{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c45453b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import SimpleITK as sitk\n",
    "import os\n",
    "import numpy as np\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from ipywidgets import interact, fixed\n",
    "import math\n",
    "import glob\n",
    "from scipy.interpolate import RegularGridInterpolator, interpn\n",
    "\n",
    "from skimage import measure\n",
    "\n",
    "import pylab as pl\n",
    "import trimesh\n",
    "from stl import mesh\n",
    "import re\n",
    "\n",
    "from scipy.spatial import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffe0dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data set\n",
    "def centeroidnp(arr):\n",
    "    \"\"\"get the centroid of a point cloud\"\"\"\n",
    "    length = arr.shape[0]\n",
    "    sum_x = np.sum(arr[:, 0])\n",
    "    sum_y = np.sum(arr[:, 1])\n",
    "    sum_z = np.sum(arr[:, 2])\n",
    "    return math.ceil(sum_x/length), \\\n",
    "            math.ceil(sum_y/length), \\\n",
    "            math.ceil(sum_z/length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9453018",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_7D(source_pc, source_center, target_center):\n",
    "    \"\"\"create a 7D pointcloud as explained in Fu et al.\"\"\"\n",
    "    v_s = np.zeros((len(source_pc), 7))\n",
    "    for i in range(len(v_s)):\n",
    "        v_ss = source_center - source_pc[i,:3]\n",
    "        v_st = target_center - source_pc[i,:3]\n",
    "        v_s[i,:3] = v_ss \n",
    "        v_s[i,3:6] = v_st\n",
    "        v_s[i,6] = source_pc[i,3]\n",
    "    return v_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b62cb489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_position_and_orientation(files_deform, files_regular):\n",
    "    \"\"\"\n",
    "    set the positioning and orientation of the deformed images so that they align\n",
    "    NOTE: this should be done in the begining when the initial deformed mhd files are created\n",
    "    \"\"\"\n",
    "    for d ,f in zip(files_deform, files_regular):\n",
    "        with open(d, 'a+') as deformed:\n",
    "            with open(f, 'r') as regular:\n",
    "                for r in regular.readlines():\n",
    "                    if \"Position\" in r: #or \"Orientation\" in r:\n",
    "                        deformed.write(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f94a9e3",
   "metadata": {},
   "source": [
    "# Make connections between vertebrae bodies and laminas with facets for XML scene in SOFA framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "292b0a84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def dist_pts(a, b):\n",
    "    return np.linalg.norm(a-b)\n",
    "\n",
    "def min_dist(points, p):\n",
    "    min_=min([dist_pts(a[:3],p) for a in points])\n",
    "    return [dist_pts(a[:3],p) for a in points].index(min_)\n",
    "\n",
    "def intersect2d(X, Y):\n",
    "        \"\"\"\n",
    "        Function to find intersection of two 2D arrays.\n",
    "        Returns index of rows in X that are common to Y.\n",
    "        \"\"\"\n",
    "        X = np.tile(X[:,:,None], (1, 1, Y.shape[0]) )\n",
    "        Y = np.swapaxes(Y[:,:,None], 0, 2)\n",
    "        Y = np.tile(Y, (X.shape[0], 1, 1))\n",
    "        eq = np.all(np.equal(X, Y), axis = 1)\n",
    "        eq = np.any(eq, axis = 1)\n",
    "        return np.nonzero(eq)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aeeba32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_bbox(position):\n",
    "    \"\"\" \n",
    "    Gets the bounding box of the object defined by the given vertices.\n",
    "\n",
    "    Arguments\n",
    "    -----------\n",
    "    position : list\n",
    "    List with the coordinates of N points (position field of Sofa MechanicalObject).\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    xmin, xmax, ymin, ymax, zmin, zmax : floats\n",
    "    min and max coordinates of the object bounding box.\n",
    "    \"\"\"\n",
    "    points_array = np.asarray(position)\n",
    "    m = np.min(points_array, axis=0)\n",
    "    xmin, ymin, zmin = m[0], m[1], m[2]\n",
    "\n",
    "    m = np.max(points_array, axis=0)\n",
    "    xmax, ymax, zmax = m[0], m[1], m[2]\n",
    "\n",
    "    return xmin, xmax, ymin, ymax, zmin, zmax\n",
    "\n",
    "def get_indices_in_bbox( positions, bbox ):\n",
    "    \"\"\"\n",
    "    Get the indices of the points falling within the specified bounding box.\n",
    "\n",
    "    Arguments\n",
    "    ----------\n",
    "    positions (list):\n",
    "    N x 3 list of points coordinates.\n",
    "    bbox (list):\n",
    "    [xmin, ymin, zmin, xmax, ymax, zmax] extremes of the bounding box.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    indices:\n",
    "    List of indices of points enclosed in the bbox.\n",
    "\n",
    "    \"\"\"\n",
    "    # bbox is in the format (xmin, ymin, zmin, xmax, ...)\n",
    "    assert len(bbox) == 6\n",
    "    indices = []\n",
    "    for i, x in enumerate( positions ):\n",
    "        if x[0] >= bbox[0] and x[0] <= bbox[3] and x[1] >= bbox[1] and x[1] <= bbox[4] and x[2] >= bbox[2] and x[2] <= bbox[5]:\n",
    "            indices.append( i )\n",
    "    return indices\n",
    "        \n",
    "\n",
    "def print_stiff_springs(vert1,vert2, bbox_v1_v2,bbox_v2_v1, s, d):\n",
    "    \"\"\"\n",
    "    vert1 and vert2: two adjecent vertebrae\n",
    "    bbox_v1_v2 and bbox_v2_v1 are the bounding boxes representing area where the \n",
    "    springs are found on the closer sides of two adjecent vertebrae\n",
    "    \"\"\"\n",
    "    idx1 = get_indices_in_bbox(vert1, bbox_v1_v2)[::5]\n",
    "    idx2 = get_indices_in_bbox(vert2, bbox_v2_v1)[::5]\n",
    "    print(\"SPRINGS: \")\n",
    "    np.random.shuffle(idx1)\n",
    "    np.random.shuffle(idx2)\n",
    "    for i,j in zip(idx1,idx2):\n",
    "        print(\"{0} {1} {2} {3} {4}  \".format(i,j,s,d,dist_pts(vert1[i],vert2[j])), end='')\n",
    "        \n",
    "def print_positions(vert1, bbox_v1_t12):\n",
    "    \"\"\"\n",
    "    this function is used for seting the fixed points on L1 and L5 simulating\n",
    "    connection with T12 and S1 respectively\n",
    "    \"\"\"\n",
    "    \n",
    "    idx1 = get_indices_in_bbox(vert1, bbox_v1_t12)[::5]\n",
    "    print(\"POSITIONS:\")\n",
    "    for i in idx1:\n",
    "        print(\"{0} {1} {2}  \".format(vert1[i][0],vert1[i][1],vert1[i][2]), end='')\n",
    "    print(\"Indexes for fixed constraint\")\n",
    "    for i,_ in enumerate(idx1):\n",
    "        print(i, end=\" \")\n",
    "    print(\"SPRINGS: \")\n",
    "    for i,j in enumerate(idx1):\n",
    "        print(\"{0} {1} {2} {3} {4}  \".format(i,j,1000,10,0.00001), end='')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647b6ba",
   "metadata": {},
   "source": [
    "## uncomment the block below to print the connections and vertices for the springs in SOFA framework AND to create biomechanical constraint files change paths and bounding boxes accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1ac51052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_vector(array, value):\n",
    "    idx = np.array([np.linalg.norm(x+y+z) for (x,y,z) in np.abs(array[:,:3]-value[:3])]).argmin()\n",
    "    return int(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e6985f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bboxes(values):\n",
    "    return [np.min(values[:,0]), np.min(values[:,1]) ,np.min(values[:,2]),\n",
    "      np.max(values[:,0]), np.max(values[:,1]),np.max(values[:,2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a8faf7b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #print bounding boxes\n",
    "# for i in range(11,23):\n",
    "#     print(\"#################################  \" + str(i))\n",
    "#     vert1 = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine\"+str(i)+\"/v1.txt\")[:,:3]\n",
    "#     vert2 = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine\"+str(i)+\"/v2.txt\")[:,:3]\n",
    "#     vert3 = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine\"+str(i)+\"/v3.txt\")[:,:3]\n",
    "#     vert4 = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine\"+str(i)+\"/v4.txt\")[:,:3]\n",
    "#     vert5 = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine\"+str(i)+\"/v5.txt\")[:,:3] \n",
    "    \n",
    "#     verts = [vert1,vert2,vert3,vert4,vert5]\n",
    "    \n",
    "    \n",
    "#     bbox_v1_t12 = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine\"+str(i)+\"/v1_t12.txt\")\n",
    "#     bbox_v1_t12 = bboxes(bbox_v1_t12)\n",
    "#     print(bbox_v1_t12)\n",
    "#     bbox_v1_v2 = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine\"+str(i)+\"/v2_v1.txt\")\n",
    "#     bbox_v1_v2 = bboxes(bbox_v1_v2)\n",
    "#     print(bbox_v1_v2)\n",
    "#     print(bbox_v1_v2)\n",
    "#     bbox_v2_v3 = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine\"+str(i)+\"/v3_v2.txt\")\n",
    "#     bbox_v2_v3 = bboxes(bbox_v2_v3)\n",
    "#     print(bbox_v2_v3)\n",
    "#     print(bbox_v2_v3)\n",
    "#     bbox_v3_v4 = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine\"+str(i)+\"/v4_v3.txt\")\n",
    "#     bbox_v3_v4 = bboxes(bbox_v3_v4)\n",
    "#     print(bbox_v3_v4)\n",
    "#     print(bbox_v3_v4)\n",
    "\n",
    "#     bbox_v4_v5 = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine\"+str(i)+\"/v5_v4.txt\")\n",
    "#     bbox_v4_v5 = bboxes(bbox_v4_v5)\n",
    "#     print(bbox_v4_v5)\n",
    "#     print(bbox_v4_v5)\n",
    "\n",
    "#     bbox_v5_s1 = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine\"+str(i)+\"/v5_s1.txt\")\n",
    "#     bbox_v5_s1 = bboxes(bbox_v5_s1)\n",
    "#     print(bbox_v5_s1)\n",
    "\n",
    "#     values = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine\"+str(i)+\"/facet12.txt\")\n",
    "#     print(bboxes(values))\n",
    "#     print(bboxes(values))\n",
    "\n",
    "#     values = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine\"+str(i)+\"/facet23.txt\")\n",
    "#     print(bboxes(values))\n",
    "#     print(bboxes(values))\n",
    "\n",
    "#     values = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine\"+str(i)+\"/facet34.txt\")\n",
    "#     print(bboxes(values))\n",
    "#     print(bboxes(values))\n",
    "\n",
    "#     values = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine\"+str(i)+\"/facet45.txt\")\n",
    "#     print(bboxes(values))\n",
    "#     print(bboxes(values))\n",
    "    \n",
    "    \n",
    "#     bboxe = [bbox_v1_t12, bbox_v1_v2,\n",
    "#           bbox_v1_v2, bbox_v2_v3, \n",
    "#           bbox_v2_v3, bbox_v3_v4,\n",
    "#           bbox_v3_v4, bbox_v4_v5,\n",
    "#           bbox_v4_v5, bbox_v5_s1]\n",
    "\n",
    "#     len_v = 0\n",
    "#     with open(\"Spine\"+str(i)+\"_biomechanical.txt\", \"w+\") as file:\n",
    "#         indices1 = get_indices_in_bbox(verts[0], bboxe[1])\n",
    "#         indices2 = get_indices_in_bbox(verts[1], bboxe[2])\n",
    "#         indices3 = get_indices_in_bbox(verts[1], bboxe[3])\n",
    "#         indices4 = get_indices_in_bbox(verts[2], bboxe[4])\n",
    "#         indices5 = get_indices_in_bbox(verts[2], bboxe[5])\n",
    "#         indices6 = get_indices_in_bbox(verts[3], bboxe[6])\n",
    "#         indices7 = get_indices_in_bbox(verts[3], bboxe[7])\n",
    "#         indices8 = get_indices_in_bbox(verts[4], bboxe[8])\n",
    "\n",
    "\n",
    "#         a1 = find_nearest_vector(verts[0],np.mean(verts[0][indices1], axis=0))\n",
    "#         a2 = find_nearest_vector(verts[1],np.mean(verts[1][indices2], axis=0))\n",
    "#         a3 = find_nearest_vector(verts[1],np.mean(verts[1][indices3], axis=0))\n",
    "#         a4 = find_nearest_vector(verts[2],np.mean(verts[2][indices4], axis=0))\n",
    "#         a5 = find_nearest_vector(verts[2],np.mean(verts[2][indices5], axis=0))\n",
    "#         a6 = find_nearest_vector(verts[3],np.mean(verts[3][indices6], axis=0))\n",
    "#         a7 = find_nearest_vector(verts[3],np.mean(verts[3][indices7], axis=0))\n",
    "#         a8 = find_nearest_vector(verts[4],np.mean(verts[4][indices8], axis=0))\n",
    "\n",
    "#         file.write(\"{0} {1} {2} {3} {4} {5} {6} {7}\".format(a1,a2,a3,a4,a5,a6,a7,a8))\n",
    "#     #SANITY CHECK\n",
    "# #     file.write(\"{0} {1} {2}\\n{3} {4} {5}\\n{6} {7} {8}\\n{9} {10} {11}\\n{12} {13} {14}\\n{15} {16} {17}\\n{18} {19} {20}\\n{21} {22} {23}\\n\"\n",
    "# #                                 .format(verts[0][a1][0],\n",
    "# #                                         verts[0][a1][1],\n",
    "# #                                         verts[0][a1][2],\n",
    "# #                                         verts[1][a2][0],\n",
    "# #                                         verts[1][a2][1],\n",
    "# #                                         verts[1][a2][2],\n",
    "# #                                          verts[1][a3][0],\n",
    "# #                                         verts[1][a3][1],\n",
    "# #                                         verts[1][a3][2],\n",
    "# #                                         verts[2][a4][0],\n",
    "# #                                         verts[2][a4][1],\n",
    "# #                                         verts[2][a4][2],\n",
    "# #                                          verts[2][a5][0],\n",
    "# #                                         verts[2][a5][1],\n",
    "# #                                         verts[2][a5][2],\n",
    "# #                                         verts[3][a6][0],\n",
    "# #                                         verts[3][a6][1],\n",
    "# #                                         verts[3][a6][2],\n",
    "# #                                          verts[3][a7][0],\n",
    "# #                                         verts[3][a7][1],\n",
    "# #                                         verts[3][a7][2],\n",
    "# #                                         verts[4][a8][0],\n",
    "# #                                         verts[4][a8][1],\n",
    "# #                                         verts[4][a8][2],\n",
    "# #                                                          ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "675d48bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vert1 = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine22/v1.txt\")[:,:3]\n",
    "vert2 = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine22/v2.txt\")[:,:3]\n",
    "vert3 = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine22/v3.txt\")[:,:3]\n",
    "vert4 = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine22/v4.txt\")[:,:3]\n",
    "vert5 = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine22/v5.txt\")[:,:3] \n",
    "\n",
    "values = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine22/v1_t12.txt\")\n",
    "bbox_v1_t12 = bboxes(values)\n",
    "values = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine22/v2_v1.txt\")\n",
    "bbox_v1_v2 = bboxes(values)\n",
    "bbox_v2_v1 = bboxes(values)\n",
    "values = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine22/v3_v2.txt\")\n",
    "bbox_v2_v3 = bboxes(values)\n",
    "bbox_v3_v2 = bboxes(values)\n",
    "values = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine22/v4_v3.txt\")\n",
    "bbox_v3_v4 = bboxes(values)\n",
    "bbox_v4_v3 = bboxes(values)\n",
    "values = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine22/v5_v4.txt\")\n",
    "bbox_v4_v5 = bboxes(values)\n",
    "bbox_v5_v4 = bboxes(values)\n",
    "values = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine22/v5_s1.txt\")\n",
    "bbox_v5_s1 = bboxes(values)\n",
    "bbox_v5_s1[0]+=0.012\n",
    "bbox_v5_s1[3]-=0.012\n",
    "\n",
    "values = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine22/facet12.txt\")\n",
    "bbox_bone_v1_v2 = bboxes(values)\n",
    "bbox_bone_v2_v1 = bboxes(values)\n",
    "values = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine22/facet23.txt\")\n",
    "bbox_bone_v2_v3 = bboxes(values)\n",
    "bbox_bone_v3_v2 = bboxes(values)\n",
    "values = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine22/facet34.txt\")\n",
    "bbox_bone_v3_v4 = bboxes(values)\n",
    "bbox_bone_v4_v3 = bboxes(values)\n",
    "values = np.loadtxt(\"/Users/janelameski/Downloads/new_spines_Jane/spine22/facet45.txt\")\n",
    "bbox_bone_v4_v5 = bboxes(values)\n",
    "bbox_bone_v5_v4 = bboxes(values)\n",
    "\n",
    "\n",
    "# print_positions(vert1, bbox_v1_t12)\n",
    "# print()\n",
    "\n",
    "# print_positions(vert5, bbox_v5_s1)\n",
    "# print()\n",
    "# # format(i,j,500,3,dist_pts(vert1[i],vert2[j])), end='')\n",
    "# print_stiff_springs(vert1, vert2, bbox_v1_v2, bbox_v2_v1,500,3)\n",
    "# print()\n",
    "# print_stiff_springs(vert2, vert3, bbox_v2_v3, bbox_v3_v2,500,3)\n",
    "# print()\n",
    "# print_stiff_springs(vert3, vert4, bbox_v3_v4, bbox_v4_v3,500,3)\n",
    "# print()\n",
    "# print_stiff_springs(vert4, vert5, bbox_v4_v5, bbox_v5_v4,500,3)\n",
    "# # format(i,j,8000,500,dist_pts(vert1[i],vert2[j])), end='')\n",
    "# print()\n",
    "# print()\n",
    "# print_stiff_springs(vert1, vert2, bbox_bone_v1_v2, bbox_bone_v2_v1,8000,500)\n",
    "# print()\n",
    "# print_stiff_springs(vert2, vert3, bbox_bone_v2_v3, bbox_bone_v3_v2,8000,500)\n",
    "# print()\n",
    "# print_stiff_springs(vert3, vert4, bbox_bone_v3_v4, bbox_bone_v4_v3,8000,500)\n",
    "# print()\n",
    "# print_stiff_springs(vert4, vert5, bbox_bone_v4_v5, bbox_bone_v5_v4,8000,500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23a42b1",
   "metadata": {},
   "source": [
    "# vtu to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79dee32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change paths accordingly\n",
    "path_vtu_files = \"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/\"\n",
    "path_txt_files = \"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/\"\n",
    "def vtu2txt(path_vtu_files, path_txt_files):\n",
    "    list_files = []\n",
    "    #check lines which start anything but numbers(in the vtu files the rows \n",
    "    #with numbers are the rows containing the point cloud)\n",
    "    regex = re.compile(\"^ *<|^  +\\d|^\\t<|^\\t\\d|^\\t-|^ +-\")\n",
    "    \n",
    "    #put all vtu files in list_files\n",
    "    for file in os.listdir(path_vtu_files):\n",
    "        if file.endswith(\".vtu\"):\n",
    "            list_files.append(file)\n",
    "    #read all files one by one\n",
    "    for file in list_files:\n",
    "        with open(path_vtu_files + file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        #filter them using the regex above\n",
    "        filtered = [i for i in lines if not regex.match(i)]\n",
    "        #write them in a file with same name but ending txt\n",
    "        with open(path_txt_files + file[:-3]+\"txt\", \"w+\") as f:\n",
    "            for l in filtered:\n",
    "                \n",
    "                x,y,z = l.split(\" \")\n",
    "                f.write(\"{0} {1} {2}\\n\".format(float(x)*1e+3,float(y)*1e+3,float(z)*1e+3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787c4bfe",
   "metadata": {},
   "source": [
    "##  uncomment below to change vtu to point clouds txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5eabca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "vtu2txt(path_vtu_files, path_txt_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f119d9e5",
   "metadata": {},
   "source": [
    "## Convert vtu to obj; Uncomment to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9326fdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import meshio\n",
    "#change path accordingly\n",
    "path_to_vtu = \"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/\"\n",
    "\n",
    "for file in os.listdir(path_to_vtu ):\n",
    "    if file.endswith(\".vtu\"):\n",
    "\n",
    "        mesh_vtu = meshio.read(path_to_vtu + file)\n",
    "    \n",
    "        mesh = meshio.Mesh(\n",
    "        mesh_vtu.points*1e3,\n",
    "        mesh_vtu.cells,\n",
    "        # Optionally provide extra data on points, cells, etc.\n",
    "        mesh_vtu.point_data,\n",
    "        # Each item in cell data must match the cells array\n",
    "        mesh_vtu.cell_data,\n",
    "        )\n",
    "        mesh.write(path_to_vtu + \"obj_files/\" + file[:-3]+ \"obj\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52747bbb",
   "metadata": {},
   "source": [
    "## obj to mhd is done using ImFusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d885f6",
   "metadata": {},
   "source": [
    "## raycast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9d13e156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raycast_spine1(image):# spine10, spine8, spine9, spine7 , spine6 , spine 5, spine4 , spine3 , spine2 , spine 1, spine 11-19, 22\n",
    "    #\n",
    "    rays = np.zeros_like(np.squeeze(np.squeeze(image)))\n",
    "    for i in range(image.shape[1]):\n",
    "        for j in range(image.shape[0]-1, 0, -1):\n",
    "            if image[j, i] != 0:\n",
    "                rays[j, i] = 1\n",
    "                break\n",
    "    return rays\n",
    "\n",
    "def raycast_spine8(image):# spine 8\n",
    "    rays = np.zeros_like(np.squeeze(np.squeeze(image)))\n",
    "    for i in range(image.shape[1]):\n",
    "        for j in range(image.shape[0]):\n",
    "            if image[j, i] != 0:\n",
    "                rays[j, i] = 1\n",
    "                break\n",
    "    return rays\n",
    "\n",
    "def raycast_spine20(image):#spine 20, 21\n",
    "    rays = np.zeros_like(np.squeeze(np.squeeze(image)))\n",
    "    for i in range(image.shape[0]):\n",
    "        for j in range(image.shape[1]-1, 0, -1):\n",
    "            if image[i, j] != 0:\n",
    "                rays[i, j] = 1\n",
    "                break\n",
    "    return rays\n",
    "\n",
    "def raycast_files(files_path, spine_id):\n",
    "    for path in os.listdir(files_path):\n",
    "        if path.endswith(\".mhd\") and path.startswith(spine_id):\n",
    "            img_mhd = sitk.ReadImage(files_path + path)\n",
    "            im = sitk.GetArrayFromImage(img_mhd)\n",
    "\n",
    "            raycasted = np.zeros_like(im)\n",
    "            \n",
    "            if \"e1\" in spine_id or \"e2_\" in spine_id or \"e3\" in spine_id or \"e4\" in spine_id or \"e6\" in spine_id or \"e7\" in spine_id or \"e10\" in spine_id or \"e22\" in spine_id:\n",
    "                for i in range(im.shape[0]):\n",
    "                    raycasted[i,...] = raycast_spine1(im[i,...])\n",
    "            elif \"e5\" in spine_id or \"e9\" in spine_id or \"e8\" in spine_id:\n",
    "                if \"e5\" in spine_id or \"e9\" in spine_id:\n",
    "                    for i in range(im.shape[2]):\n",
    "                        raycasted[...,i] = raycast_spine1(im[...,i])#spine 5, 9\n",
    "                elif \"e8\" in spine_id :\n",
    "                    for i in range(im.shape[2]):\n",
    "                        raycasted[...,i] = raycast_spine1(im[...,i])#spine 8,\n",
    "            else:\n",
    "                for i in range(im.shape[1]):\n",
    "                    raycasted[:,i,:] = raycast_spine20(im[:,i,:])\n",
    "            \n",
    "            raycasted_img = sitk.GetImageFromArray(raycasted)\n",
    "            sitk.WriteImage(raycasted_img,  files_path + \"raycasted_\" +path[:-3] + \"mhd\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc9315",
   "metadata": {},
   "source": [
    "## Uncomment the cell below to create raycasted mhd files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "de4846a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_label_maps = \"/Users/janelameski/Desktop/labelMaps/\"\n",
    "list_of_spine_ids = [i[:-4] for i in os.listdir(path_to_label_maps) if i.startswith(\"spine2\") and i.endswith(\"mhd\")]\n",
    "for spine_id in list_of_spine_ids:\n",
    "    raycast_files(path_to_label_maps, spine_id)\n",
    "    list_raycasted = sorted([path_to_label_maps + i for i in os.listdir(path_to_label_maps) if i.startswith(\"raycasted_\"+spine_id) and i.endswith(\".mhd\")])\n",
    "    list_non_raycasted = sorted([path_to_label_maps + i for i in os.listdir(path_to_label_maps) if i.startswith(spine_id) and i.endswith(\".mhd\")])\n",
    "    set_position_and_orientation(list_raycasted, list_non_raycasted)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7adaad",
   "metadata": {},
   "source": [
    "## mhd to .txt is done using ImFusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b294553",
   "metadata": {},
   "source": [
    "# Prepare spine data .txt to .npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ae846cfd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_files = \"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/\" + \"txtFiles/\"\n",
    "def obtain_file_id(p):\n",
    "    \"\"\"\n",
    "    IMPORTANT: make sure that the path to files folder (p) does not contain \"_\" except in the name of the \n",
    "    files themselves\n",
    "    \"\"\"\n",
    "    splits = p.split(\"_\")\n",
    "    #in case the label of the spine is double digit\n",
    "    if splits[0][-2:].isnumeric():\n",
    "        #dealing with Source point clouds \"spine11_vert10.txt\"\n",
    "        if len(splits)==2:\n",
    "            s_id = splits[0][-7:] + splits[1][5]\n",
    "        #this is the case only for several target files such as spine11_vert2_1.txt\n",
    "        elif len(splits)==3 and splits[2].endswith(\".txt\"):\n",
    "            s_id = splits[0][-7:] + \"_\" +splits[2][0]    \n",
    "        #for all other target files such as \"spine11_vert5_1_0\"\n",
    "        else:\n",
    "            s_id = splits[0][-7:] + \"_\"+splits[2]+\"_\"+splits[3][0]\n",
    "    else:  \n",
    "         #dealing with Source point clouds \"spine1_vert10.txt\"\n",
    "        if len(splits)==2:\n",
    "            s_id = splits[0][-6:] + splits[1][5]\n",
    "        #this is the case only for several target files such as spine1_vert2_1.txt\n",
    "        elif len(splits)==3 and splits[2].endswith(\".txt\"):\n",
    "            s_id = splits[0][-6:] + \"_\" +splits[2][0]    \n",
    "        #for all other target files such as \"spine1_vert5_1_0\"\n",
    "        else:\n",
    "            s_id = splits[0][-6:] + \"_\"+splits[2]+\"_\"+splits[3][0]\n",
    "    return s_id\n",
    "\n",
    "def obtain_indices_raycasted_original_pc(spine_target, r_target):\n",
    "    \"\"\"\n",
    "    find indices in spine_target w.r.t. r_target such that they are the closest points between the two \n",
    "    point clouds\n",
    "    \"\"\"\n",
    "    kdtree=KDTree(spine_target[:,:3])\n",
    "    dist,points=kdtree.query(r_target[:,:3],1)\n",
    "\n",
    "    return list(set(points))\n",
    "\n",
    "def create_source_target_with_vertebra_label(source_pc, target_pc, vert):\n",
    "    \"\"\"\n",
    "    source_pc: source point cloud\n",
    "    target_pc: target point cloud\n",
    "    vert: [1-5] for [L1-L5] vertebra respectively\n",
    "    \n",
    "    this function is to create source and target point clouds with label for each vertebra\n",
    "    \"\"\"\n",
    "    \n",
    "    source = np.ones((source_pc.shape[0], source_pc.shape[1]+1))\n",
    "    source[:, :3]=source_pc\n",
    "    source[:, 3] = source[:, 3]*vert\n",
    "    target = np.ones((target_pc.shape[0], target_pc.shape[1]+1))\n",
    "    target[:, :3]=target_pc\n",
    "    target[:, 3]= target[:, 3]*vert\n",
    "    \n",
    "    return source, target\n",
    "\n",
    "def create_source_target_flow_spine(source_pc, target_pc):\n",
    "    \"\"\"\n",
    "    source_pc: source point cloud\n",
    "    target_pc: target point cloud\n",
    "    vert: [1-5] for [L1-L5] vertebra respectively\n",
    "    \n",
    "    this function is to create source and target point clouds with 7D\n",
    "    where the point clouds are centered.\n",
    "    \"\"\"\n",
    "    \n",
    "#     source_pc, target_pc = create_source_target_with_vertebra_label(source_pc, target_pc, vert)\n",
    "\n",
    "    centroid_source = centeroidnp(source_pc)\n",
    "    centroid_target = centeroidnp(target_pc)\n",
    "    \n",
    "    source_7d = create_7D(source_pc, centroid_source, centroid_target)\n",
    "    target_7d = create_7D(target_pc, centroid_source, centroid_target)\n",
    "    \n",
    "    flow = target_7d[:,:3]-source_7d[:,:3]\n",
    "    \n",
    "    return source_7d, target_7d, flow\n",
    "\n",
    "\n",
    "\n",
    "def write_source_target_flow_as_npz(pa, num):\n",
    "    \"\"\"\n",
    "    USE THIS METHOD TO CREATE FULL SPINE DATA AS .npz\n",
    "    \n",
    "    pa: path to vertebrae data set with name of files: \n",
    "    \"spine{num_spine}_vert{num_vert}_{SOFA_experiment}_{Nx20_iterations}.txt\"\n",
    "    num: which patient does the spine belong to labeled from 1-10\n",
    "    \n",
    "    returns a list of spine point clouds containing flow source target, \n",
    "    and another list with the constraints\n",
    "    \"\"\"\n",
    "    source_rgx = re.compile(\"^spine\"+str(num)+\"_vert.0\")\n",
    "    l1_rgx = re.compile(\"^spine\"+str(num)+\"_vert1_\")\n",
    "    l2_rgx = re.compile(\"^spine\"+str(num)+\"_vert2_\")\n",
    "    l3_rgx = re.compile(\"^spine\"+str(num)+\"_vert3_\")\n",
    "    l4_rgx = re.compile(\"^spine\"+str(num)+\"_vert4_\")\n",
    "    l5_rgx = re.compile(\"^spine\"+str(num)+\"_vert5_\")\n",
    "    \n",
    "    source_list = []\n",
    "    #if the path to the biomechanical constraints is different, add it accordingly \n",
    "    constraint_list = np.loadtxt(\"Spine\"+str(num)+\"_biomechanical.txt\")\n",
    "    l1=[]\n",
    "    l2=[]\n",
    "    l3=[]\n",
    "    l4=[]\n",
    "    l5=[]\n",
    "    \n",
    "    #list the files matching the regex above to their name\n",
    "    for file in os.listdir(pa):\n",
    "        if file.endswith(\".txt\") and source_rgx.match(file):\n",
    "            source_list.append(os.path.join(pa, file))\n",
    "        if file.endswith(\".txt\") and l1_rgx.match(file):\n",
    "            l1.append(os.path.join(pa, file))\n",
    "        if file.endswith(\".txt\") and l2_rgx.match(file):\n",
    "            l2.append(os.path.join(pa, file))\n",
    "        if file.endswith(\".txt\") and l3_rgx.match(file):\n",
    "            l3.append(os.path.join(pa, file))\n",
    "        if file.endswith(\".txt\") and l4_rgx.match(file):\n",
    "            l4.append(os.path.join(pa, file))\n",
    "        if file.endswith(\".txt\") and l5_rgx.match(file):\n",
    "            l5.append(os.path.join(pa, file))\n",
    "    \n",
    "    source_list = sorted(source_list)\n",
    "    targets = (sorted(l1),sorted(l2),sorted(l3),sorted(l4),sorted(l5))\n",
    "\n",
    "    list_of_triples = []\n",
    "    list_of_spines = []\n",
    "    for i, t in enumerate(targets):\n",
    "        \n",
    "        for p in t:\n",
    "            sour, tar = create_source_target_with_vertebra_label(np.loadtxt(source_list[i]), np.loadtxt(p), i+1)\n",
    "            t_id = obtain_file_id(p)\n",
    "            s_id = obtain_file_id(source_list[i])\n",
    "            list_of_triples.append([sour, tar, t_id])\n",
    "\n",
    "        list_of_spines.append(list_of_triples)\n",
    "        list_of_triples=[]\n",
    "\n",
    "    #for every vertebra in the combined files\n",
    "    for t1,t2,t3,t4,t5 in zip(list_of_spines[0], list_of_spines[1], list_of_spines[2],\n",
    "                         list_of_spines[3], list_of_spines[4]):\n",
    "        \n",
    "        #create the whole spines (source and target)\n",
    "        spine_source = np.concatenate((t1[0],t2[0],t3[0],t4[0],t5[0]))\n",
    "        spine_target = np.concatenate((t1[1],t2[1],t3[1],t4[1],t5[1]))\n",
    "        \n",
    "        source, target, flow = create_source_target_flow_spine(spine_source, spine_target)\n",
    "    \n",
    "        target_id = t1[2]\n",
    "        \n",
    "        np.savez_compressed(pa + target_id + \".npz\",\n",
    "                                    flow=flow,\n",
    "                                    pc1=source,\n",
    "                                    pc2=target,\n",
    "                                    cstPts=constraint_list)\n",
    "\n",
    "\n",
    "def combine_vertebrae_with_label(pa, num, raycasted_path):\n",
    "    \"\"\"\n",
    "    USE THIS METHOD TO CREATE RAY CASTED SPINE DATA AS .npz \n",
    "    \n",
    "    pa: path to vertebrae data set with name of files:\n",
    "    \"spine{num_spine}_vert{num_vert}_{SOFA_experiment}_{Nx20_iterations}.txt\"\n",
    "    num: which spine we are transforming\n",
    "    raycasted_path: path where the raycasts are saved as\n",
    "    \"\"\"\n",
    "    \n",
    "    #select the file names according to whether they are source \"spine1_vert10.txt\" or target \"spine1_vert1_1_0\"\n",
    "    source_rgx = re.compile(\"^spine\"+str(num)+\"_vert.0\")\n",
    "    l1_rgx = re.compile(\"^spine\"+str(num)+\"_vert1_\")\n",
    "    l2_rgx = re.compile(\"^spine\"+str(num)+\"_vert2_\")\n",
    "    l3_rgx = re.compile(\"^spine\"+str(num)+\"_vert3_\")\n",
    "    l4_rgx = re.compile(\"^spine\"+str(num)+\"_vert4_\")\n",
    "    l5_rgx = re.compile(\"^spine\"+str(num)+\"_vert5_\")\n",
    "    \n",
    "    l1=[]\n",
    "    l2=[]\n",
    "    l3=[]\n",
    "    l4=[]\n",
    "    l5=[]\n",
    "    \n",
    "    source_list = []\n",
    "    \n",
    "    #list the files matching the regex above to their name\n",
    "    for file in os.listdir(pa):\n",
    "        if file.endswith(\".txt\") and source_rgx.match(file):\n",
    "            source_list.append(os.path.join(pa, file))\n",
    "        if file.endswith(\".txt\") and l1_rgx.match(file):\n",
    "            l1.append(os.path.join(pa, file))\n",
    "        if file.endswith(\".txt\") and l2_rgx.match(file):\n",
    "            l2.append(os.path.join(pa, file))\n",
    "        if file.endswith(\".txt\") and l3_rgx.match(file):\n",
    "            l3.append(os.path.join(pa, file))\n",
    "        if file.endswith(\".txt\") and l4_rgx.match(file):\n",
    "            l4.append(os.path.join(pa, file))\n",
    "        if file.endswith(\".txt\") and l5_rgx.match(file):\n",
    "            l5.append(os.path.join(pa, file))\n",
    "            \n",
    "    source_list = sorted(source_list)\n",
    "    targets = (sorted(l1),sorted(l2),sorted(l3),sorted(l4),sorted(l5))\n",
    "\n",
    "    list_of_triplets = []\n",
    "    list_of_spines = []\n",
    "    \n",
    "\n",
    "    #combine the files in a list of lists containing [source, target, source_id, target_id]\n",
    "    for i, t in enumerate(targets):\n",
    "        for p in t:\n",
    "            sour, tar = create_source_target_with_vertebra_label(np.loadtxt(source_list[i]), np.loadtxt(p), i+1)\n",
    "            t_id = obtain_file_id(p)\n",
    "            s_id = obtain_file_id(source_list[i])\n",
    "            list_of_triplets.append([sour,tar,s_id, t_id])\n",
    "        list_of_spines.append(list_of_triplets)\n",
    "        list_of_triplets = []\n",
    "    spine_source = []\n",
    "    spine_target = []\n",
    "    \n",
    "    #load the biomechanical constraint point indices\n",
    "    bio_constraints = np.loadtxt(\"Spine\"+str(num)+\"_biomechanical.txt\")\n",
    "    \n",
    "    #for every vertebra in the combined files\n",
    "    for t1,t2,t3,t4,t5 in zip(list_of_spines[0], list_of_spines[1], list_of_spines[2],\n",
    "                         list_of_spines[3], list_of_spines[4]):\n",
    "        \n",
    "        #create the whole spines (source and target)\n",
    "        spine_source = np.concatenate((t1[0],t2[0],t3[0],t4[0],t5[0]))\n",
    "        spine_target = np.concatenate((t1[1],t2[1],t3[1],t4[1],t5[1]))\n",
    "        \n",
    "        #create the flow\n",
    "        spine_flow = spine_target[:,:3] - spine_source[:,:3]\n",
    "        \n",
    "        #the id of the spine\n",
    "        source_id = t1[2]\n",
    "        target_id = t1[3]\n",
    "        \n",
    "        #find the respective points for the biomechanical constraint point indices\n",
    "        bio_constraints = [int(i) for i in bio_constraints]\n",
    "        bio_points = [find_nearest_vector(spine_source, t1[0][bio_constraints[0]]),\n",
    "                      find_nearest_vector(spine_source, t2[0][bio_constraints[1]]),\n",
    "                      find_nearest_vector(spine_source, t2[0][bio_constraints[2]]),\n",
    "                      find_nearest_vector(spine_source, t3[0][bio_constraints[3]]),\n",
    "                      find_nearest_vector(spine_source, t3[0][bio_constraints[4]]),\n",
    "                      find_nearest_vector(spine_source, t4[0][bio_constraints[5]]),\n",
    "                      find_nearest_vector(spine_source, t4[0][bio_constraints[6]]),\n",
    "                      find_nearest_vector(spine_source, t5[0][bio_constraints[7]])]\n",
    "        \n",
    "\n",
    "        #load the extracted point clouds from the .mhd ray casts\n",
    "        r_source = np.loadtxt(raycasted_path + \"raycasted_\" + source_id + \".txt\")\n",
    "        r_target = np.loadtxt(raycasted_path + \"raycasted_\" + target_id + \".txt\")\n",
    "        \n",
    "        raycasted_source_idcs = obtain_indices_raycasted_original_pc(spine_source, r_source)\n",
    "        raycasted_target_idcs = obtain_indices_raycasted_original_pc(spine_target, r_target)\n",
    "        #add the biomechanical constraint points in the raycasted_source because they are probably\n",
    "        #not part of the point cloud\n",
    "        raycasted_source_idcs.extend(bio_points)\n",
    "        \n",
    "        #select only the ray casted points from the original source and target\n",
    "        raycasted_source = spine_source[raycasted_source_idcs, :]\n",
    "        raycasted_target = spine_target[raycasted_target_idcs, :]\n",
    "        raycasted_source_flow = np.zeros_like(raycasted_source)\n",
    "        raycasted_source_flow[:,:3] = raycasted_source[:,:3] + spine_flow[raycasted_source_idcs, :]\n",
    "        raycasted_source_flow[:,3] = raycasted_source[:,3]\n",
    "        \n",
    "        #prepare data into 7D \n",
    "        centroid_source = centeroidnp(raycasted_source)\n",
    "        centroid_target = centeroidnp(raycasted_target)\n",
    "        centroid_source_flow = centeroidnp(raycasted_source_flow)\n",
    "        \n",
    "        source_7d = create_7D(raycasted_source, centroid_source, centroid_source_flow)\n",
    "        target_7d = create_7D(raycasted_target, centroid_source, centroid_target)\n",
    "        flow_source_7d = create_7D(raycasted_source_flow, centroid_source, centroid_source_flow)\n",
    "\n",
    "        flow = flow_source_7d[:,:3]-source_7d[:,:3]\n",
    "        \n",
    "    \n",
    "        #find the biomechanical points in every vertebra \n",
    "        surface1 = np.copy(raycasted_source[:,3])\n",
    "        L1 = raycasted_source[np.argwhere(surface1 == 1.).squeeze()]\n",
    "        L2 = raycasted_source[np.argwhere(surface1 == 2.).squeeze()]\n",
    "        L3 = raycasted_source[np.argwhere(surface1 == 3.).squeeze()]\n",
    "        L4 = raycasted_source[np.argwhere(surface1 == 4.).squeeze()]\n",
    "        L5 = raycasted_source[np.argwhere(surface1 == 5.).squeeze()]\n",
    "        \n",
    "        #create the .npz file\n",
    "        np.savez_compressed(pa + \"raycasted_\"+ target_id + \".npz\",\n",
    "                                    flow=flow,\n",
    "                                    pc1=source_7d,\n",
    "                                    pc2=target_7d,\n",
    "                                    cstPts=np.array([L1.shape[0]-1,\n",
    "                                                    L2.shape[0]-1,L2.shape[0]-2,\n",
    "                                                    L3.shape[0]-1,L3.shape[0]-2,\n",
    "                                                   L4.shape[0]-1,L4.shape[0]-2,\n",
    "                                                   L5.shape[0]-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0adc83",
   "metadata": {},
   "source": [
    "## uncomment the cell below and change paths accordingly to create files from full spine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "325e8d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_npz = \"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/\" + \"txtFiles/\"\n",
    "\n",
    "# for i in range(22):\n",
    "#     write_source_target_flow_as_npz(path_to_npz, i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee02679d",
   "metadata": {},
   "source": [
    "## Uncomment the cell below and change paths accordingly to create npz training files from raycasted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d0d54cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_npz = \"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/\" + \"txtFiles/\"\n",
    "# path_to_label_maps = \"/Users/janelameski/Desktop/labelMaps/\"\n",
    "\n",
    "# for i in range(22):\n",
    "#     combine_vertebrae_with_label(path_to_npz, i+1, path_to_label_maps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
