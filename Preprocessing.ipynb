{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c45453b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import SimpleITK as sitk\n",
    "import os\n",
    "import numpy as np\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from ipywidgets import interact, fixed\n",
    "import math\n",
    "import glob\n",
    "from scipy.interpolate import RegularGridInterpolator, interpn\n",
    "\n",
    "from skimage import measure\n",
    "\n",
    "import pylab as pl\n",
    "import trimesh\n",
    "from stl import mesh\n",
    "import re\n",
    "\n",
    "from scipy.spatial import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffe0dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data set\n",
    "def centeroidnp(arr):\n",
    "    \"\"\"get the centroid of a point cloud\"\"\"\n",
    "    length = arr.shape[0]\n",
    "    sum_x = np.sum(arr[:, 0])\n",
    "    sum_y = np.sum(arr[:, 1])\n",
    "    sum_z = np.sum(arr[:, 2])\n",
    "    return math.ceil(sum_x/length), \\\n",
    "            math.ceil(sum_y/length), \\\n",
    "            math.ceil(sum_z/length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9453018",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_7D(source_pc, source_center, target_center):\n",
    "    \"\"\"create a 7D pointcloud as explained in Fu et al.\"\"\"\n",
    "    v_s = np.zeros((len(source_pc), 7))\n",
    "    for i in range(len(v_s)):\n",
    "        v_ss = source_center - source_pc[i,:3]\n",
    "        v_st = target_center - source_pc[i,:3]\n",
    "        v_s[i,:3] = v_ss \n",
    "        v_s[i,3:6] = v_st\n",
    "        v_s[i,6] = source_pc[i,3]\n",
    "    return v_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b62cb489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_position_and_orientation(files_deform, files_regular):\n",
    "    \"\"\"\n",
    "    set the positioning and orientation of the deformed images so that they align\n",
    "    NOTE: this should be done in the begining when the initial deformed mhd files are created\n",
    "    \"\"\"\n",
    "    for d ,f in zip(files_deform, files_regular):\n",
    "        with open(d, 'a+') as deformed:\n",
    "            with open(f, 'r') as regular:\n",
    "                for r in regular.readlines():\n",
    "                    if \"Position\" in r: #or \"Orientation\" in r:\n",
    "                        deformed.write(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f94a9e3",
   "metadata": {},
   "source": [
    "# Make connections between vertebrae bodies and laminas with facets for XML scene in SOFA framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "292b0a84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def dist_pts(a, b):\n",
    "    return np.linalg.norm(a-b)\n",
    "\n",
    "def min_dist(points, p):\n",
    "    min_=min([dist_pts(a[:3],p) for a in points])\n",
    "    return [dist_pts(a[:3],p) for a in points].index(min_)\n",
    "\n",
    "def intersect2d(X, Y):\n",
    "        \"\"\"\n",
    "        Function to find intersection of two 2D arrays.\n",
    "        Returns index of rows in X that are common to Y.\n",
    "        \"\"\"\n",
    "        X = np.tile(X[:,:,None], (1, 1, Y.shape[0]) )\n",
    "        Y = np.swapaxes(Y[:,:,None], 0, 2)\n",
    "        Y = np.tile(Y, (X.shape[0], 1, 1))\n",
    "        eq = np.all(np.equal(X, Y), axis = 1)\n",
    "        eq = np.any(eq, axis = 1)\n",
    "        return np.nonzero(eq)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aeeba32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_bbox(position):\n",
    "    \"\"\" \n",
    "    Gets the bounding box of the object defined by the given vertices.\n",
    "\n",
    "    Arguments\n",
    "    -----------\n",
    "    position : list\n",
    "    List with the coordinates of N points (position field of Sofa MechanicalObject).\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    xmin, xmax, ymin, ymax, zmin, zmax : floats\n",
    "    min and max coordinates of the object bounding box.\n",
    "    \"\"\"\n",
    "    points_array = np.asarray(position)\n",
    "    m = np.min(points_array, axis=0)\n",
    "    xmin, ymin, zmin = m[0], m[1], m[2]\n",
    "\n",
    "    m = np.max(points_array, axis=0)\n",
    "    xmax, ymax, zmax = m[0], m[1], m[2]\n",
    "\n",
    "    return xmin, xmax, ymin, ymax, zmin, zmax\n",
    "\n",
    "def get_indices_in_bbox( positions, bbox ):\n",
    "    \"\"\"\n",
    "    Get the indices of the points falling within the specified bounding box.\n",
    "\n",
    "    Arguments\n",
    "    ----------\n",
    "    positions (list):\n",
    "    N x 3 list of points coordinates.\n",
    "    bbox (list):\n",
    "    [xmin, ymin, zmin, xmax, ymax, zmax] extremes of the bounding box.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    indices:\n",
    "    List of indices of points enclosed in the bbox.\n",
    "\n",
    "    \"\"\"\n",
    "    # bbox is in the format (xmin, ymin, zmin, xmax, ...)\n",
    "    assert len(bbox) == 6\n",
    "    indices = []\n",
    "    for i, x in enumerate( positions ):\n",
    "        if x[0] >= bbox[0] and x[0] <= bbox[3] and x[1] >= bbox[1] and x[1] <= bbox[4] and x[2] >= bbox[2] and x[2] <= bbox[5]:\n",
    "            indices.append( i )\n",
    "    return indices\n",
    "        \n",
    "\n",
    "def print_stiff_springs(vert1,vert2, bbox_v1_v2,bbox_v2_v1, s, d):\n",
    "    \"\"\"\n",
    "    vert1 and vert2: two adjecent vertebrae\n",
    "    bbox_v1_v2 and bbox_v2_v1 are the bounding boxes representing area where the \n",
    "    springs are found on the closer sides of two adjecent vertebrae\n",
    "    \"\"\"\n",
    "    idx1 = get_indices_in_bbox(vert1, bbox_v1_v2)[::5]\n",
    "    idx2 = get_indices_in_bbox(vert2, bbox_v2_v1)[::5]\n",
    "    print(\"SPRINGS: \")\n",
    "    np.random.shuffle(idx1)\n",
    "    np.random.shuffle(idx2)\n",
    "    for i,j in zip(idx1,idx2):\n",
    "        print(\"{0} {1} {2} {3} {4}  \".format(i,j,s,d,dist_pts(vert1[i],vert2[j])), end='')\n",
    "        \n",
    "def print_positions(vert1, bbox_v1_t12):\n",
    "    \"\"\"\n",
    "    this function is used for seting the fixed points on L1 and L5 simulating\n",
    "    connection with T12 and S1 respectively\n",
    "    \"\"\"\n",
    "    \n",
    "    idx1 = get_indices_in_bbox(vert1, bbox_v1_t12)[::5]\n",
    "    print(\"POSITIONS:\")\n",
    "    for i in idx1:\n",
    "        print(\"{0} {1} {2}  \".format(vert1[i][0],vert1[i][1],vert1[i][2]), end='')\n",
    "    print(\"Indexes for fixed constraint\")\n",
    "    for i,_ in enumerate(idx1):\n",
    "        print(i, end=\" \")\n",
    "    print(\"SPRINGS: \")\n",
    "    for i,j in enumerate(idx1):\n",
    "        print(\"{0} {1} {2} {3} {4}  \".format(i,j,1000,10,0.00001), end='')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647b6ba",
   "metadata": {},
   "source": [
    "## uncomment the block below to print the connections and vertices for the springs in SOFA framework change paths and bounding boxes accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "675d48bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vert1 = np.loadtxt(\"DataJane/Spine10/v1.txt\")[:,:3]\n",
    "# vert2 = np.loadtxt(\"DataJane/Spine10/v2.txt\")[:,:3]\n",
    "# vert3 = np.loadtxt(\"DataJane/Spine10/v3.txt\")[:,:3]\n",
    "# vert4 = np.loadtxt(\"DataJane/Spine10/v4.txt\")[:,:3]\n",
    "# vert5 = np.loadtxt(\"DataJane/Spine10/v5.txt\")[:,:3] \n",
    "              \n",
    "# bbox_v1_t12 = [-0.0239, 0.2275, -0.0591, 0.0629, 0.2764, -0.0442]\n",
    "# bbox_v1_v2 = [-0.0239, 0.2275, -0.08, 0.0629, 0.272, -0.062]\n",
    "# bbox_v2_v1 = [-0.0239, 0.2275, -0.087, 0.0629, 0.266, -0.062]\n",
    "# bbox_v2_v3 = [-0.0239, 0.2265, -0.11, 0.0629, 0.262, -0.09]\n",
    "# bbox_v3_v2 = [-0.0239, 0.2, -0.115, 0.0629, 0.253, -0.09]\n",
    "# bbox_v3_v4 = [-0.0239, 0.2, -0.14, 0.0629, 0.248, -0.122]\n",
    "# bbox_v4_v3 = [-0.0239, 0.2, -0.15, 0.0629, 0.245, -0.122]\n",
    "# bbox_v4_v5 = [-0.0239, 0.2, -0.18, 0.0629, 0.245, -0.16]\n",
    "# bbox_v5_v4 = [-0.0239, 0.19, -0.187, 0.0629, 0.24, -0.16]\n",
    "# bbox_v5_s1 = [-0.0239, 0.19, -0.214, 0.0629, 0.24, -0.195]\n",
    "\n",
    "# bbox_bone_v1_v2 = [0.001, 0.272, -0.1, 0.0529, 0.295, -0.071]\n",
    "# bbox_bone_v2_v1 = [-0.0139, 0.267, -0.1, 0.0529, 0.287, -0.06]\n",
    "# bbox_bone_v2_v3 = [-0.0139, 0.26, -0.125, 0.0529, 0.283, -0.1]\n",
    "# bbox_bone_v3_v2 = [-0.0139, 0.255, -0.125, 0.0529, 0.277, -0.1]\n",
    "# bbox_bone_v3_v4 = [-0.0139, 0.245, -0.154, 0.0529, 0.268, -0.13]\n",
    "# bbox_bone_v4_v3 = [-0.0139, 0.245, -0.154, 0.0529, 0.268, -0.122]\n",
    "# bbox_bone_v4_v5 = [-0.0139, 0.24, -0.187, 0.0529, 0.268, -0.16]\n",
    "# bbox_bone_v5_v4 = [-0.0139, 0.24, -0.187, 0.0529, 0.268, -0.16]\n",
    "\n",
    "# print_positions(vert1, bbox_v1_t12)\n",
    "# print()\n",
    "# print_positions(vert5, bbox_v5_s1)\n",
    "# print()\n",
    "# # format(i,j,500,3,dist_pts(vert1[i],vert2[j])), end='')\n",
    "# print_stiff_springs(vert1, vert2, bbox_v1_v2, bbox_v2_v1,500,3)\n",
    "# print()\n",
    "# print_stiff_springs(vert2, vert3, bbox_v2_v3, bbox_v3_v2,500,3)\n",
    "# print()\n",
    "# print_stiff_springs(vert3, vert4, bbox_v3_v4, bbox_v4_v3,500,3)\n",
    "# print()\n",
    "# print_stiff_springs(vert4, vert5, bbox_v4_v5, bbox_v5_v4,500,3)\n",
    "# # format(i,j,8000,500,dist_pts(vert1[i],vert2[j])), end='')\n",
    "# print()\n",
    "# print()\n",
    "# print_stiff_springs(vert1, vert2, bbox_bone_v1_v2, bbox_bone_v2_v1,8000,500)\n",
    "# print()\n",
    "# print_stiff_springs(vert2, vert3, bbox_bone_v2_v3, bbox_bone_v3_v2,8000,500)\n",
    "# print()\n",
    "# print_stiff_springs(vert3, vert4, bbox_bone_v3_v4, bbox_bone_v4_v3,8000,500)\n",
    "# print()\n",
    "# print_stiff_springs(vert4, vert5, bbox_bone_v4_v5, bbox_bone_v5_v4,8000,500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9df29f",
   "metadata": {},
   "source": [
    "## Take points for Biomechanical constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3debf845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_point_idx(point_cloud, input_point):\n",
    "    \"\"\"\n",
    "    Returns the index closest point to <input_point> in the input <point_cloud>\n",
    "    \"\"\"\n",
    "    idx = np.array([np.linalg.norm(x+y+z) for (x,y,z) in np.abs(point_cloud[:,:3]-input_point[:3])]).argmin()\n",
    "    return int(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12f57699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bboxes(values):\n",
    "    \"\"\"\n",
    "    Returns the bounding boxes from a file created manualy in ImFusion\n",
    "    \"\"\"\n",
    "    return [np.min(values[:,0]), np.min(values[:,1]) ,np.min(values[:,2]),\n",
    "      np.max(values[:,0]), np.max(values[:,1]),np.max(values[:,2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "086ff9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vert1 = np.loadtxt(\"DataJane/Spine10/v1.txt\")[:,:3]\n",
    "# vert2 = np.loadtxt(\"DataJane/Spine10/v2.txt\")[:,:3]\n",
    "# vert3 = np.loadtxt(\"DataJane/Spine10/v3.txt\")[:,:3]\n",
    "# vert4 = np.loadtxt(\"DataJane/Spine10/v4.txt\")[:,:3]\n",
    "# vert5 = np.loadtxt(\"DataJane/Spine10/v5.txt\")[:,:3]\n",
    "\n",
    "# verts = [vert1,vert2,vert3,vert4,vert5]\n",
    "# #bounding boxes of the vertebrae, produced manually and saved in the above cell for \n",
    "# #every spine separately\n",
    "# bbox_v1_t12 = [-0.0239, 0.2275, -0.0591, 0.0629, 0.2764, -0.0442]\n",
    "# bbox_v1_v2 = [-0.0239, 0.2275, -0.08, 0.0629, 0.272, -0.062]\n",
    "# bbox_v2_v1 = [-0.0239, 0.2275, -0.087, 0.0629, 0.266, -0.062]\n",
    "# bbox_v2_v3 = [-0.0239, 0.2265, -0.11, 0.0629, 0.262, -0.09]\n",
    "# bbox_v3_v2 = [-0.0239, 0.2, -0.115, 0.0629, 0.253, -0.09]\n",
    "# bbox_v3_v4 = [-0.0239, 0.2, -0.14, 0.0629, 0.248, -0.122]\n",
    "# bbox_v4_v3 = [-0.0239, 0.2, -0.15, 0.0629, 0.245, -0.122]\n",
    "# bbox_v4_v5 = [-0.0239, 0.2, -0.18, 0.0629, 0.245, -0.16]\n",
    "# bbox_v5_v4 = [-0.0239, 0.19, -0.187, 0.0629, 0.24, -0.16]\n",
    "# bbox_v5_s1 = [-0.0239, 0.19, -0.214, 0.0629, 0.24, -0.195]\n",
    "\n",
    "# bboxes = [bbox_v1_t12, bbox_v1_v2,\n",
    "#           bbox_v2_v1, bbox_v2_v3, \n",
    "#           bbox_v3_v2, bbox_v3_v4,\n",
    "#           bbox_v4_v3, bbox_v4_v5,\n",
    "#           bbox_v5_v4, bbox_v5_s1]\n",
    "\n",
    "# len_v = 0\n",
    "# with open(\"Spine10_biomechanical.txt\", \"w+\") as file:\n",
    "#     indices1 = get_indices_in_bbox(verts[0], bboxes[1])\n",
    "#     indices2 = get_indices_in_bbox(verts[1], bboxes[2])\n",
    "#     indices3 = get_indices_in_bbox(verts[1], bboxes[3])\n",
    "#     indices4 = get_indices_in_bbox(verts[2], bboxes[4])\n",
    "#     indices5 = get_indices_in_bbox(verts[2], bboxes[5])\n",
    "#     indices6 = get_indices_in_bbox(verts[3], bboxes[6])\n",
    "#     indices7 = get_indices_in_bbox(verts[3], bboxes[7])\n",
    "#     indices8 = get_indices_in_bbox(verts[4], bboxes[8])\n",
    "\n",
    "    \n",
    "#     a1 = find_nearest_vector(verts[0],np.mean(verts[0][indices1], axis=0))\n",
    "#     a2 = find_nearest_vector(verts[1],np.mean(verts[1][indices2], axis=0))\n",
    "#     a3 = find_nearest_vector(verts[1],np.mean(verts[1][indices3], axis=0))\n",
    "#     a4 = find_nearest_vector(verts[2],np.mean(verts[2][indices4], axis=0))\n",
    "#     a5 = find_nearest_vector(verts[2],np.mean(verts[2][indices5], axis=0))\n",
    "#     a6 = find_nearest_vector(verts[3],np.mean(verts[3][indices6], axis=0))\n",
    "#     a7 = find_nearest_vector(verts[3],np.mean(verts[3][indices7], axis=0))\n",
    "#     a8 = find_nearest_vector(verts[4],np.mean(verts[4][indices8], axis=0))\n",
    "    \n",
    "#     file.write(\"{0} {1} {2} {3} {4} {5} {6} {7}\".format(a1,a2,a3,a4,a5,a6,a7,a8))\n",
    "    #SANITY CHECK\n",
    "#     file.write(\"{0} {1} {2}\\n{3} {4} {5}\\n{6} {7} {8}\\n{9} {10} {11}\\n{12} {13} {14}\\n{15} {16} {17}\\n{18} {19} {20}\\n{21} {22} {23}\\n\"\n",
    "#                                 .format(verts[0][a1][0],\n",
    "#                                         verts[0][a1][1],\n",
    "#                                         verts[0][a1][2],\n",
    "#                                         verts[1][a2][0],\n",
    "#                                         verts[1][a2][1],\n",
    "#                                         verts[1][a2][2],\n",
    "#                                          verts[1][a3][0],\n",
    "#                                         verts[1][a3][1],\n",
    "#                                         verts[1][a3][2],\n",
    "#                                         verts[2][a4][0],\n",
    "#                                         verts[2][a4][1],\n",
    "#                                         verts[2][a4][2],\n",
    "#                                          verts[2][a5][0],\n",
    "#                                         verts[2][a5][1],\n",
    "#                                         verts[2][a5][2],\n",
    "#                                         verts[3][a6][0],\n",
    "#                                         verts[3][a6][1],\n",
    "#                                         verts[3][a6][2],\n",
    "#                                          verts[3][a7][0],\n",
    "#                                         verts[3][a7][1],\n",
    "#                                         verts[3][a7][2],\n",
    "#                                         verts[4][a8][0],\n",
    "#                                         verts[4][a8][1],\n",
    "#                                         verts[4][a8][2],\n",
    "#                                                          ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d74e22",
   "metadata": {},
   "source": [
    "## 2 Creating the dataset\n",
    "The following scripts describe how to reorder and preprocess the .vtu data, in output from the sofa framework to a\n",
    "data format compatible with the network loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc733d9d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2. 1 Reordering Files generated from sofa:\n",
    "The output files from sofa are reordered such that DbFolder (i.e. where the\n",
    "dataset is saved) is organized as follows: DbFolder/Spine<i>/timestamp<t> with <i> being the spine id and <t> the\n",
    "simulation timestamp. Each of such folder contains 5 files, one corresponding to each vertebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a03db12d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from shutil import copy2\n",
    "\n",
    "def extract_spine_id(filename):\n",
    "    \"\"\"\n",
    "    Given a file, it extracts the id of the spine.\n",
    "\n",
    "    Example 1.\n",
    "\n",
    "    .. code-block:: console\n",
    "    >> filename = <spine_folder>\\sspine1_vert1_1_0.txt\n",
    "    >> extract_spine_id(filename)\n",
    "    spine_1\n",
    "\n",
    "    Example 2:\n",
    "    >> filename = spine1_vert1_0.txt\n",
    "    >> extract_spine_id(filename)\n",
    "    spine_1\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    filename = os.path.split(filename)[-1]\n",
    "\n",
    "    return filename.split(\"_\")[0]\n",
    "\n",
    "def extract_vertebra_id(filename):\n",
    "    \"\"\"\n",
    "    Given a file, it extracts the id of the vertebra\n",
    "\n",
    "    Example 1.\n",
    "\n",
    "    .. code-block:: console\n",
    "    >> filename = <spine_folder>\\sspine1_vert1_1_0.txt\n",
    "    >> extract_vertebra_id(filename)\n",
    "    vert1\n",
    "\n",
    "    Example 2:\n",
    "    >> filename = spine1_vert1_0.txt\n",
    "    >> extract_vertebra_id(filename)\n",
    "    vert1\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    filename = os.path.split(filename)[-1]\n",
    "    return filename.split(\"_\")[1][0:5]\n",
    "\n",
    "\n",
    "def extract_timestamp_id(filename):\n",
    "    \"\"\"\n",
    "    Given a file, it extracts the id of the timestamp\n",
    "\n",
    "    Example 1.\n",
    "\n",
    "    .. code-block:: console\n",
    "    >> filename = <spine_folder>\\sspine1_vert1_1_0.txt\n",
    "    >> extract_timestamp_id(filename)\n",
    "    1_0\n",
    "\n",
    "    Example 2:\n",
    "    >> filename = spine1_vert1_1_0.txt\n",
    "    >> extract_timestamp_id(filename)\n",
    "    1_0\n",
    "\n",
    "    \"\"\"\n",
    "    filename = os.path.split(filename)[-1]\n",
    "    spine_id = extract_spine_id(filename)\n",
    "    vertebra_id = extract_vertebra_id(filename)\n",
    "\n",
    "    timestamp_id = filename.replace(spine_id + \"_\" + vertebra_id, \"\")\n",
    "    timestamp_id = timestamp_id.split(\".\")[0]\n",
    "\n",
    "    return timestamp_id\n",
    "\n",
    "\n",
    "def order_files_in_fold(src_filepath, dst_folder, copy=True):\n",
    "    \"\"\"\n",
    "    Given a certain filepath, it copies (or moves it if copy==False) it in the correct folder location\n",
    "\n",
    "    Example:\n",
    "\n",
    "        .. code-block:: console\n",
    "        >> src_filepath = VTU_output_from_SOFA\\spine1_vert1_1_0.vtu\n",
    "        >> dst_folder = DB_Folder\n",
    "        >> patient_id = spine1\n",
    "        >> timestamp_id = _1_0\n",
    "\n",
    "        by running:\n",
    "        >> order_files_in_fold(src_filepath, dst_folder, patient_id, timestamp_id, copy=True)\n",
    "\n",
    "        The file will be copied in DB_Folder/spine_1/ts_1_0/spine1_vert1_1_0.vtu\n",
    "    \"\"\"\n",
    "\n",
    "    patient_id = extract_spine_id(src_filepath)\n",
    "    timestamp_id = extract_timestamp_id(src_filepath)\n",
    "\n",
    "    src_filename = os.path.split(src_filepath)[-1]\n",
    "\n",
    "    # minor correction in naming - the first deformation file misses the timestamp 0\n",
    "    if timestamp_id.count(\"_\") == 1:\n",
    "        timestamp_id = \"_0\" + timestamp_id\n",
    "\n",
    "    dst_folder = os.path.join(dst_folder, patient_id, \"ts\" + timestamp_id)\n",
    "\n",
    "    if not os.path.exists(dst_folder):\n",
    "        os.makedirs(dst_folder)\n",
    "\n",
    "    dst_filepath = os.path.join(dst_folder, src_filename)\n",
    "\n",
    "    if copy:\n",
    "        copy2(src_filepath, dst_filepath)\n",
    "\n",
    "def reorder_vtu_files(src_vtu_dir, dst_vtu_dir, copy=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Given a certain filepath containing the .vtu (or any other format) obtained by sofa with the following naming:\n",
    "    spine<spineId>_vert<vertId><timestamp_id>.vtu, it reorders them in the dst_vtu_dir, such that a given file\n",
    "    spine<spineId>_vert<vertId><timestamp_id>.vtu is stored in\n",
    "    dst_vtu_dir/spine<spineId>/<timestamp_id>/spine<spineId>_vert<vertId><timestamp_id>.vtu\n",
    "\n",
    "    Example:\n",
    "        given a src_vtu_dir containing\n",
    "        ['spine1_vert10.vtu', 'spine1_vert1_0.vtu', 'spine1_vert1_1.vtu', 'spine1_vert1_10_0.vtu',\n",
    "        'spine1_vert1_10_1.vtu', ..., 'spine1_vert5_7_1.vtu', 'spine1_vert5_8_0.vtu', 'spine1_vert5_8_1.vtu',\n",
    "        'spine1_vert5_9_0.vtu', 'spine1_vert5_9_1.vtu''spine2_vert10.vtu', 'spine2_vert1_10_0.vtu',\n",
    "        'spine2_vert1_11_0.vtu', 'spine2_vert1_12_0.vtu', 'spine2_vert1_13_0.vtu', ... ]\n",
    "\n",
    "    The function copies (or moves if copy == False) them\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: console\n",
    "            >> src_filepath = VTU_output_from_SOFA\\spine1_vert1_1_0.vtu\n",
    "            >> dst_folder = DB_Folder\n",
    "            >> patient_id = spine1\n",
    "            >> timestamp_id = _1_0\n",
    "\n",
    "    by running:\n",
    "    >> order_files_in_fold(src_filepath, dst_folder, patient_id, timestamp_id, copy=True)\n",
    "\n",
    "    The files are reordered in the dst_vtu_dir as follows:\n",
    "    dst_vtu_dir\\spine1\\ts10\\spine1_vert10.vtu\n",
    "    dst_vtu_dir\\spine1\\ts10\\spine1_vert20.vtu\n",
    "    dst_vtu_dir\\spine1\\ts10\\spine1_vert30.vtu\n",
    "    dst_vtu_dir\\spine1\\ts10\\spine1_vert40.vtu\n",
    "    dst_vtu_dir\\spine1\\ts10\\spine1_vert50.vtu\n",
    "                    ...\n",
    "    dst_vtu_dir\\spine1\\ts10\\spine1_vert1_10_0.vtu\n",
    "    dst_vtu_dir\\spine1\\ts10\\spine1_vert2_10_0.vtu\n",
    "    dst_vtu_dir\\spine1\\ts10\\spine1_vert3_10_0.vtu\n",
    "    dst_vtu_dir\\spine1\\ts10\\spine1_vert4_10_0.vtu\n",
    "    dst_vtu_dir\\spine1\\ts10\\spine1_vert5_10_0.vtu\n",
    "                    ...\n",
    "    dst_vtu_dir\\spine2\\ts10\\spine1_vert1_10_0.vtu\n",
    "    dst_vtu_dir\\spine2\\ts10\\spine1_vert2_10_0.vtu\n",
    "    dst_vtu_dir\\spine2\\ts10\\spine1_vert3_10_0.vtu\n",
    "    dst_vtu_dir\\spine2\\ts10\\spine1_vert4_10_0.vtu\n",
    "    dst_vtu_dir\\spine2\\ts10\\spine1_vert5_10_0.vtu\n",
    "                    ...\n",
    "    \"\"\"\n",
    "\n",
    "    for file in [item for item in os.listdir(src_vtu_dir) if \".vtu\" in item]:\n",
    "        order_files_in_fold(src_filepath=os.path.join(src_vtu_dir, file),\n",
    "                            dst_folder=dst_vtu_dir,\n",
    "                            copy=copy)\n",
    "\n",
    "reorder_vtu_files(src_vtu_dir=\"E:/NAS/jane_project/VTU_output_from_SOFA\",\n",
    "                  dst_vtu_dir=\"E:/NAS/jane_project/reordered_vtu_files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23a42b1",
   "metadata": {},
   "source": [
    "### 2. 2 vtu to txt\n",
    "Generating .txt point cloud files from the Dataset folder containing the vtu files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79dee32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def vtu2txt(src_vtu_dir, dst_txt_dir):\n",
    "    \"\"\"\n",
    "    Converts a .vtu file in output from sofa to a .txt point cloud file by copying the points coordinates to the\n",
    "    .txt file\n",
    "\n",
    "    :param src_vtu_dir: str: path to the folder containing the .vtu files, ordered according to the reorder_vtu_files\n",
    "    script (e.g. where files are saved according to:\n",
    "     vtu_dir\\spine<spineId>\\ts<timestampId>\\spine<spineId>_vert<vertId>_<timestampId>.vtu)\n",
    "    :param dst_txt_dir: str: path to the folder where the .txt files will be saved, according to the usual folder\n",
    "    structure dst_txt_dir\\spine<spineId>\\ts<timestampId>\\spine<spineId>_vert<vertId>_<timestampId>.txt)\n",
    "    \"\"\"\n",
    "    #check lines which start anything but numbers(in the vtu files the rows \n",
    "    #with numbers are the rows containing the point cloud)\n",
    "    regex = re.compile(\"^ *<|^  +\\d|^\\t<|^\\t\\d|^\\t-|^ +-\")\n",
    "\n",
    "    patient_id_list = [item for item in os.listdir(src_vtu_dir) if \"spine\" in item]\n",
    "\n",
    "    for patient_id in patient_id_list:\n",
    "        timestamp_list = [item for item in os.listdir(os.path.join(src_vtu_dir, patient_id)) if \"ts\" in item]\n",
    "\n",
    "        for timestamp in timestamp_list:\n",
    "            file_list = [item for item in os.listdir(os.path.join(src_vtu_dir, patient_id, timestamp))\n",
    "                         if item.endswith(\".vtu\")]\n",
    "\n",
    "            #write them in a file with same name but ending txt\n",
    "            dst_folder = os.path.join(dst_txt_dir, patient_id, timestamp)\n",
    "            if not os.path.exists(dst_folder):\n",
    "                os.makedirs(dst_folder)\n",
    "\n",
    "            #read all files one by one\n",
    "            for file in file_list:\n",
    "                with open(os.path.join(src_vtu_dir, patient_id, timestamp, file), \"r\") as f:\n",
    "                    lines = f.readlines()\n",
    "                #filter them using the regex above\n",
    "                filtered = [i for i in lines if not regex.match(i)]\n",
    "                dst_filename = os.path.join(dst_folder, file.replace(\".vtu\", \".txt\"))\n",
    "\n",
    "                with open(dst_filename, \"w+\") as f:\n",
    "                    for l in filtered:\n",
    "\n",
    "                        x,y,z = l.split(\" \")\n",
    "                        f.write(\"{0} {1} {2}\\n\".format(float(x)*1e+3,float(y)*1e+3,float(z)*1e+3))\n",
    "\n",
    "vtu2txt(src_vtu_dir = \"E:/NAS/jane_project/reordered_vtu_files\",\n",
    "        dst_txt_dir = \"E:/NAS/jane_project/txt_files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f119d9e5",
   "metadata": {},
   "source": [
    "### 2. 3 vtu to obj\n",
    "Generating .obj meshes from the Dataset folder containing the vtu files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9326fdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import meshio\n",
    "\n",
    "def vtu2obj(src_vtu_dir, dst_obj_dir):\n",
    "    \"\"\"\n",
    "    Converts a .vtu file in output from sofa to a .obj file containing the point cloud mesh.\n",
    "\n",
    "    :param src_vtu_dir: str: path to the folder containing the .vtu files, ordered according to the reorder_vtu_files\n",
    "    script (e.g. where files are saved according to:\n",
    "     vtu_dir\\spine<spineId>\\ts<timestampId>\\spine<spineId>_vert<vertId>_<timestampId>.vtu)\n",
    "    :param dst_obj_dir: str: path to the folder where the .txt files will be saved, according to the usual folder\n",
    "    structure dst_obj_dir\\spine<spineId>\\ts<timestampId>\\spine<spineId>_vert<vertId>_<timestampId>.obj)\n",
    "    \"\"\"\n",
    "\n",
    "    patient_id_list = [item for item in os.listdir(src_vtu_dir) if \"spine\" in item]\n",
    "\n",
    "    for patient_id in patient_id_list:\n",
    "        timestamp_list = [item for item in os.listdir(os.path.join(src_vtu_dir, patient_id)) if \"ts\" in item]\n",
    "\n",
    "        for timestamp in timestamp_list:\n",
    "            file_list = [item for item in os.listdir(os.path.join(src_vtu_dir, patient_id, timestamp))\n",
    "                         if item.endswith(\".vtu\")]\n",
    "\n",
    "            #write them in a file with same name but ending txt\n",
    "            dst_folder = os.path.join(dst_obj_dir, patient_id, timestamp)\n",
    "            if not os.path.exists(dst_folder):\n",
    "                os.makedirs(dst_folder)\n",
    "\n",
    "            for file in file_list:\n",
    "                mesh_vtu = meshio.read(os.path.join(src_vtu_dir, patient_id, timestamp, file))\n",
    "\n",
    "                mesh = meshio.Mesh(\n",
    "                    mesh_vtu.points*1e3,\n",
    "                    mesh_vtu.cells,\n",
    "                    # Optionally provide extra data on points, cells, etc.\n",
    "                    mesh_vtu.point_data,\n",
    "                    # Each item in cell data must match the cells array\n",
    "                    mesh_vtu.cell_data,\n",
    "                    )\n",
    "\n",
    "                dst_filename = os.path.join(dst_folder, file.replace(\".vtu\", \".obj\"))\n",
    "                mesh.write(dst_filename)\n",
    "\n",
    "vtu2obj(src_vtu_dir = \"E:/NAS/jane_project/reordered_vtu_files\",\n",
    "        dst_obj_dir = \"E:/NAS/jane_project/obj_files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52747bbb",
   "metadata": {},
   "source": [
    "### 2.4 Generating the label maps objects from the .obj files using in ImFusion.\n",
    "To generate the .mhd label maps from the ImFusion we run the imfusion_workspace/create_mhd_spines_from_obj.iws\n",
    "using the batch file that can be generated with the script below.\n",
    "The generated .mhd labelmaps will be saved in the dst_mhd_dir, according to the previously described data structure\n",
    "(i.e. (<dst_mhd_dir>\\spine<spine_id>\\<timestamp_id>.mhd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aee890c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def generate_obj2mhd_batch_file(src_obj_dir, dst_mhd_dir, batch_file_path):\n",
    "    \"\"\"\n",
    "    Generates the batch file for the imfusion_workspaces/create_mhd_spines_from_obj.iws imfusion workspace.\n",
    "    Example\n",
    "    .. code-block:: text\n",
    "        INPUT1;INPUT2;INPUT3;INPUT4;INPUT5;OUTPUT\n",
    "        obj_dir\\spine<spineId>\\ts<timestampId>\\spine<spineId>_vert1_<timestampId>.obj;  \\\n",
    "            obj_dir\\spine<spineId>\\ts<timestampId>\\spine<spineId>_vert2_<timestampId>.obj; \\\n",
    "            obj_dir\\spine<spineId>\\ts<timestampId>\\spine<spineId>_vert3_<timestampId>.obj; \\\n",
    "            obj_dir\\spine<spineId>\\ts<timestampId>\\spine<spineId>_vert4_<timestampId>.obj; \\\n",
    "            obj_dir\\spine<spineId>\\ts<timestampId>\\spine<spineId>_vert5_<timestampId>.obj; \\\n",
    "            mhd_dir\\spine<spineId>\\ts<timestampId>.mhd; \\\n",
    "         ...\n",
    "\n",
    "    \"\"\"\n",
    "    patient_id_list = [item for item in os.listdir(src_obj_dir) if \"spine\" in item]\n",
    "\n",
    "    fid = open(batch_file_path, 'w')\n",
    "    fid.write('INPUT1;INPUT2;INPUT3;INPUT4;INPUT5;OUTPUT')\n",
    "    for patient_id in patient_id_list:\n",
    "        timestamp_list = [item for item in os.listdir(os.path.join(src_obj_dir, patient_id)) if \"ts\" in item]\n",
    "\n",
    "        for timestamp in timestamp_list:\n",
    "            file_list = [item for item in os.listdir(os.path.join(src_obj_dir, patient_id, timestamp))\n",
    "                         if item.endswith(\".obj\")]\n",
    "\n",
    "            if not os.path.exists(os.path.join(dst_mhd_dir, patient_id)):\n",
    "                os.makedirs(os.path.join(dst_mhd_dir, patient_id))\n",
    "\n",
    "            mhd_filepath = os.path.join(dst_mhd_dir, patient_id, timestamp + \".mhd\")\n",
    "\n",
    "            fid.write(\"\\n\" + \";\".join([os.path.normpath(os.path.join(src_obj_dir, patient_id, timestamp, file))\n",
    "                                 for file in file_list]) + \";\" + mhd_filepath)\n",
    "\n",
    "generate_obj2mhd_batch_file(src_obj_dir=\"E:/NAS/jane_project/obj_files\",\n",
    "                            dst_mhd_dir=\"E:/NAS/jane_project/mhd_files\",\n",
    "                            batch_file_path=\"imfusion_workspaces/obj2mhd.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d885f6",
   "metadata": {},
   "source": [
    "### 2.5 Raycast the generated .mhd labelmap\n",
    "The script reads the labelmaps and generate the ray-casted .mhd file. The file are saved in the save_root, according\n",
    "to the previously described data_structure (<save_root>\\spine<spine_id>\\<timestamp_id>.mhd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d13e156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "\n",
    "def ray_cast_slice(image, spine_id=\"\"):\n",
    "    \"\"\"\n",
    "    Generate a ray-casted version of the input slice, in the rows direction (arrows indicating ray-casting direction).\n",
    "\n",
    "    -> __________________________________________________\n",
    "    -> |                                                 |\n",
    "    -> |                                                 |\n",
    "    -> |                                                 |\n",
    "    -> |                                                 |\n",
    "    -> |                                                 |\n",
    "    -> |                                                 |\n",
    "    -> |_________________________________________________|\n",
    "\n",
    "    Only for spine_8, the ray casting is done from right to left instead of left to right\n",
    "    spine_20 and spine_21 are special case where the slice direction is the same as spine_8 but the dimmensions \n",
    "    on which we iterate are different\n",
    "\n",
    "    __________________________________________________\n",
    "    |                                                 | <-\n",
    "    |                                                 | <-\n",
    "    |                                                 | <-\n",
    "    |                                                 | <-\n",
    "    |                                                 | <-\n",
    "    |                                                 | <-\n",
    "    |_________________________________________________| <-\n",
    "    \"\"\"\n",
    "\n",
    "    rays = np.zeros_like(np.squeeze(np.squeeze(image)))\n",
    "    if spine_id == \"spine8\":\n",
    "\n",
    "        j_range = range(image.shape[0])\n",
    "\n",
    "        for i in range(image.shape[1]):\n",
    "            for j in j_range:\n",
    "                if image[j, i] != 0:\n",
    "                    rays[j, i] = 1\n",
    "                    break\n",
    "    elif spine_id == \"spine20\" or spine_id == \"spine21\":\n",
    "        j_range = range(image.shape[1]-1, 0, -1)\n",
    "\n",
    "        for i in range(image.shape[0]):\n",
    "            for j in j_range:\n",
    "                if image[i, j] != 0:\n",
    "                    rays[i, j] = 1\n",
    "                    break\n",
    "    else:\n",
    "\n",
    "        j_range = range(image.shape[0]-1, 0, -1)\n",
    "        for i in range(image.shape[1]):\n",
    "            for j in j_range:\n",
    "                if image[j, i] != 0:\n",
    "                    rays[j, i] = 1\n",
    "                    break\n",
    "\n",
    "    return rays\n",
    "\n",
    "\n",
    "def ray_cast_data(data_path, spine_id):\n",
    "    \"\"\"\n",
    "    Ray cast all the data in the spine dataset\n",
    "\n",
    "    :param: data_path: str: The path to the .mhd file\n",
    "    :param: spine_id:str: The spine id (the raycasting direction is selected depending on the spine_id\n",
    "    \"\"\"\n",
    "\n",
    "    assert data_path.endswith(\".mhd\")\n",
    "\n",
    "    img_mhd = sitk.ReadImage(data_path)\n",
    "    im = sitk.GetArrayFromImage(img_mhd)\n",
    "\n",
    "    raycasted = np.zeros_like(im)\n",
    "\n",
    "    if spine_id in [\"spine1\", \"spine2\", \"spine3\", \"spine4\", \"spine6\", \"spine7\", \"spine10\",\n",
    "                   \"spine11\", \"spine12\", \"spine13\", \"spine14\", \"spine16\", \"spine17\",\"spine22\",\n",
    "                   \"spine15\", \"spine18\", \"spine19\"]:\n",
    "\n",
    "        for i in range(im.shape[0]):\n",
    "            raycasted[i,...] = ray_cast_slice(im[i,...], spine_id)\n",
    "\n",
    "    elif spine_id in [\"spine5\", \"spine9\", \"spine8\"]:\n",
    "        for i in range(im.shape[2]):\n",
    "            raycasted[...,i] = ray_cast_slice(im[...,i], spine_id) #spine 5, 9\n",
    "\n",
    "    else:\n",
    "        for i in range(im.shape[1]):\n",
    "            raycasted[:,i,:] = ray_cast_slice(im[:,i,:], spine_id)\n",
    "\n",
    "    # Setting the position and orientation of the ray-casted image\n",
    "    raycasted_img = sitk.GetImageFromArray(raycasted)\n",
    "    raycasted_img.SetDirection(img_mhd.GetDirection())\n",
    "    raycasted_img.SetOrigin(img_mhd.GetOrigin())\n",
    "\n",
    "    return raycasted_img\n",
    "\n",
    "\n",
    "def ray_cast_files(data_root, save_root):\n",
    "    \"\"\"\n",
    "    Ray-casts all the files contained in the dataroot directory and saved according to:\n",
    "    <data_root>\\spine<spineId>\\ts<timestampId>.mhd\n",
    "\n",
    "    :param: data_root: str: The path to the data, which must be saved according to:\n",
    "        <data_root>\\spine<spineId>\\ts<timestampId>.mhd\n",
    "    :param: save_root: the path where the raycasted data will be saved, according to:\n",
    "        <save_root>\\spine<spineId>\\raycasted_ts<timestampId>.mhd\n",
    "    \"\"\"\n",
    "\n",
    "    spine_ids =[item for item in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, item)) and\n",
    "                \"spine\" in item]\n",
    "\n",
    "    for spine_id in spine_ids:\n",
    "\n",
    "        spine_folder_path = os.path.join(data_root, spine_id)\n",
    "\n",
    "        save_spine_folder = os.path.join(save_root, spine_id)\n",
    "        if not os.path.exists(save_spine_folder):\n",
    "            os.makedirs(save_spine_folder)\n",
    "\n",
    "        for file in [item for item in os.listdir(spine_folder_path) if item.endswith(\".mhd\")]:\n",
    "\n",
    "            raycasted_img = ray_cast_data(data_path=os.path.join(spine_folder_path, file),\n",
    "                                      spine_id=spine_id)\n",
    "\n",
    "            save_path = os.path.join(save_spine_folder, \"raycasted_\" + file.split(\".\")[0] + \".mhd\")\n",
    "            sitk.WriteImage(raycasted_img,  save_path)\n",
    "\n",
    "\n",
    "ray_cast_files(data_root=\"E:/NAS/jane_project/mhd_files\",\n",
    "               save_root=\"E:/NAS/jane_project/mhd_files_raycasted\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7adaad",
   "metadata": {},
   "source": [
    "### 2.6. Convert the generated .mhd label maps to .txt point cloud file.\n",
    "mhd to .txt is done using the ImFusion workspace imfusion_workspaces/labelmap2pc.iws.\n",
    "The batch file to be used with the imfusion workspace can be generated with the script below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc395f47",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def generate_mhd2pc_batch_file(src_labelmaps_dir, dst_pcs_dir, batch_file_path):\n",
    "    \"\"\"\n",
    "    Generate the script to generate the batch file to be used for launching the imfusion_workspaces/labelmap2pc.iws.\n",
    "\n",
    "    :param: src_labelmaps_dir: str: The path to the labelmaps\n",
    "    :param: dst_pcs_dir: str: The directory where the point cloud files will be saved\n",
    "    :param: batch_file_path: str: The path where the (imfusion) .txt batch file will be generated\n",
    "\n",
    "    Example:\n",
    "\n",
    "        .. code-block:: text\n",
    "            INPUTMHD;OUTPUTPC\n",
    "            <src_labelmaps_dir>\\spine<spineId>\\raycasted_ts<timestampId>.mhd;<dst_pcs_dir>\\spine<spineId>\\raycasted_ts<timestampId>.txt\n",
    "                                        ...\n",
    "\n",
    "    \"\"\"\n",
    "    spine_ids = os.listdir(src_labelmaps_dir)\n",
    "\n",
    "    fid = open(batch_file_path, \"w\")\n",
    "\n",
    "    fid.write(\"INPUTMHD;OUTPUTPC\")\n",
    "\n",
    "    for spine_id in spine_ids:\n",
    "\n",
    "        #it appeared in my pc i had to hard code it @Jane\n",
    "        if spine_id == \".DS_Store\":\n",
    "            continue\n",
    "        dst_spine_id_folder = os.path.join(dst_pcs_dir, spine_id)\n",
    "        if not os.path.exists(dst_spine_id_folder):\n",
    "            os.makedirs(dst_spine_id_folder)\n",
    "        \n",
    "        for file in [item for item in os.listdir(os.path.join(src_labelmaps_dir, spine_id)) if \".mhd\" in item]:\n",
    "\n",
    "            input_mhd = os.path.join(src_labelmaps_dir, spine_id, file)\n",
    "            output_pc = os.path.join(dst_pcs_dir, spine_id, file.replace(\".mhd\", \".txt\"))\n",
    "\n",
    "            fid.write(\"\\n\" + input_mhd + \";\" + output_pc)\n",
    "\n",
    "    fid.close()\n",
    "\n",
    "generate_mhd2pc_batch_file(src_labelmaps_dir=\"E:/NAS/jane_project/mhd_files_raycasted\",\n",
    "                           dst_pcs_dir=\"E:/NAS/jane_project/txt_files_raycasted\",\n",
    "                           batch_file_path=\"imfusion_workspaces/labelmap2pc.txt\")\n",
    "\n",
    "# todo add imfusion workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb1ae65",
   "metadata": {},
   "source": [
    "# 2.7 Generate the batch file for ultrasound-label pairs generation for u-net training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def get_subject_split(spine_id, training_splits):\n",
    "    for split in training_splits.keys():\n",
    "        if spine_id in training_splits[split]:\n",
    "            return split\n",
    "\n",
    "def generate_us_labels_batch_file(src_labels_dir, src_ultrasound_dir, output_dir, batch_file_path, training_splits):\n",
    "    \"\"\"\n",
    "    Generate the script to generate the batch file to be used for launching the imfusion_workspaces/labelmap2pc.iws.\n",
    "\n",
    "    :param: src_labelmaps_dir: str: The path to the labelmaps\n",
    "    :param: dst_pcs_dir: str: The directory where the point cloud files will be saved\n",
    "    :param: batch_file_path: str: The path where the (imfusion) .txt batch file will be generated\n",
    "\n",
    "    Example:\n",
    "\n",
    "        .. code-block:: text\n",
    "            INPUTMHD;OUTPUTPC\n",
    "            <src_labelmaps_dir>\\spine<spineId>\\raycasted_ts<timestampId>.mhd;<dst_pcs_dir>\\spine<spineId>\\raycasted_ts<timestampId>.txt\n",
    "                                        ...\n",
    "\n",
    "    \"\"\"\n",
    "    assert \"train\" in training_splits.keys() and \"val\" in training_splits.keys() and \"test\" in training_splits.keys()\n",
    "\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        if not os.path.exists(os.path.join(output_dir, split)):\n",
    "            os.makedirs(os.path.join(output_dir, split))\n",
    "\n",
    "    spine_ids = os.listdir(src_labels_dir)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    fid = open(batch_file_path, \"w\")\n",
    "    fid.write(\"USNAME;LABELNAME;INPUTLABEL;INPUTUS;OUTPUTPATH\")\n",
    "\n",
    "    for spine_id in spine_ids:\n",
    "        #it appeared in my pc i had to hard code it @Jane\n",
    "        if \"spine\" not in spine_id:\n",
    "            continue\n",
    "\n",
    "        split = get_subject_split(spine_id, training_splits)\n",
    "        if split is None:\n",
    "            print(\"subject not in any training split\")\n",
    "            continue\n",
    "\n",
    "        save_path = os.path.join(output_dir, split)\n",
    "\n",
    "        ts_list = [item.split(\".\")[0].replace(\"_labelmap\", \"\") for item in\n",
    "                   os.listdir(os.path.join(src_labels_dir, spine_id))\n",
    "                   if \".imf\" in item]\n",
    "\n",
    "        for ts in ts_list:\n",
    "\n",
    "            input_labels_path = os.path.join(src_labels_dir, spine_id, ts + \"_labelmap\" + \".imf\")\n",
    "            input_us_path = os.path.join(src_ultrasound_dir, spine_id, ts + \".imf\")\n",
    "            image_name = spine_id + \"_\" + ts + \"_\"\n",
    "            label_name = spine_id + \"_\" + ts + \"_label_\"\n",
    "\n",
    "            fid.write(\"\\n\" + image_name + \";\" + label_name + \";\" + input_labels_path + \";\" +\n",
    "                      input_us_path + \";\" + save_path)\n",
    "\n",
    "    fid.close()\n",
    "\n",
    "train_spines = [\"spine\" + str(i) for i in range(1, 21)]\n",
    "val_spines = [\"spine21\"]\n",
    "test_spines = [\"spine22\"]\n",
    "generate_us_labels_batch_file(src_labels_dir=\"E:/NAS/jane_project/simulated_us_labelmaps\",\n",
    "                              src_ultrasound_dir=\"E:/NAS/jane_project/simulated_us\",\n",
    "                              output_dir=\"E:/NAS/jane_project/segmentation_network_data/full_labels\",\n",
    "                              batch_file_path=\"imfusion_workspaces/us_label_segmentation_data.txt\",\n",
    "                              training_splits = {\"train\": train_spines,\n",
    "                                                 \"val\": val_spines,\n",
    "                                                 \"test\": test_spines\n",
    "                                                 })\n",
    "print(\"done\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.8 Raycast simulated us labels for network training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_12428/4133654899.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     38\u001B[0m generate_raycasted_db(db_path=\"E:/NAS/jane_project/segmentation_network_data/full_labels\",\n\u001B[1;32m---> 39\u001B[1;33m                       output_dir=\"E:/NAS/jane_project/segmentation_network_data/ray_casted_labels\")\n\u001B[0m\u001B[0;32m     40\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[1;31m# 2.9 Prepare spine data .txt to .npz (to be used as an input to the network)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_12428/4133654899.py\u001B[0m in \u001B[0;36mgenerate_raycasted_db\u001B[1;34m(db_path, output_dir)\u001B[0m\n\u001B[0;32m     15\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0msplit\u001B[0m \u001B[1;32min\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;34m\"train\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"val\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"test\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 17\u001B[1;33m         \u001B[0msave_dir\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutput_dir\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msplit\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     18\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexists\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msave_dir\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m             \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmakedirs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msave_dir\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os\n",
    "from PIL import Image\n",
    "from shutil import copy2\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "def ray_cast_image(image):\n",
    "    rays = np.zeros_like(np.squeeze(np.squeeze(image)))\n",
    "    j_range = range(image.shape[0])\n",
    "\n",
    "    for i in range(image.shape[1]):\n",
    "        for j in j_range:\n",
    "            if image[j, i] != 12:\n",
    "                rays[j, i] = 1\n",
    "                break\n",
    "\n",
    "    return rays\n",
    "\n",
    "\n",
    "def generate_raycasted_db(db_path, output_dir):\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "\n",
    "        save_dir = os.path.join(output_dir, split)\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        split_path = os.path.join(db_path, split)\n",
    "\n",
    "        us_images = [item for item in os.listdir(split_path) if \"label\" not in item]\n",
    "        us_labels = [item for item in os.listdir(split_path) if \"label\" in item]\n",
    "\n",
    "        for (image_name, label_name) in zip(us_images, us_labels):\n",
    "\n",
    "            label = np.array(Image.open(os.path.join(split_path, label_name)))\n",
    "            ray_casted_labels = ray_cast_image(label)\n",
    "\n",
    "            ray_casted_labels = cv2.dilate(ray_casted_labels, np.ones((10, 10)), iterations=1)\n",
    "\n",
    "            image_save_path = os.path.join(save_dir, image_name)\n",
    "            label_save_path = os.path.join(save_dir, label_name)\n",
    "\n",
    "            copy2(os.path.join(split_path, image_name), image_save_path)\n",
    "            Image.fromarray(ray_casted_labels).save(label_save_path)\n",
    "\n",
    "\n",
    "generate_raycasted_db(db_path=\"E:/NAS/jane_project/segmentation_network_data/full_labels\",\n",
    "                      output_dir=\"E:/NAS/jane_project/segmentation_network_data/ray_casted_labels\")\n",
    "\n",
    "# 2.9 Prepare spine data .txt to .npz (to be used as an input to the network)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae846cfd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/NAS/jane_project/spine_constraints\\Spine16_biomechanical.txt\n",
      "E:/NAS/jane_project/spine_constraints\\Spine17_biomechanical.txt\n",
      "E:/NAS/jane_project/spine_constraints\\Spine18_biomechanical.txt\n",
      "E:/NAS/jane_project/spine_constraints\\Spine19_biomechanical.txt\n",
      "E:/NAS/jane_project/spine_constraints\\Spine2_biomechanical.txt\n",
      "E:/NAS/jane_project/spine_constraints\\Spine20_biomechanical.txt\n",
      "E:/NAS/jane_project/spine_constraints\\Spine21_biomechanical.txt\n",
      "E:/NAS/jane_project/spine_constraints\\Spine22_biomechanical.txt\n",
      "E:/NAS/jane_project/spine_constraints\\Spine3_biomechanical.txt\n",
      "E:/NAS/jane_project/spine_constraints\\Spine4_biomechanical.txt\n",
      "E:/NAS/jane_project/spine_constraints\\Spine5_biomechanical.txt\n",
      "E:/NAS/jane_project/spine_constraints\\Spine6_biomechanical.txt\n",
      "E:/NAS/jane_project/spine_constraints\\Spine7_biomechanical.txt\n",
      "E:/NAS/jane_project/spine_constraints\\Spine8_biomechanical.txt\n",
      "E:/NAS/jane_project/spine_constraints\\Spine9_biomechanical.txt\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import visualization_utils as utils\n",
    "from scipy.spatial import KDTree\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "list_files = \"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/\" + \"txtFiles/\"\n",
    "\n",
    "class Point:\n",
    "    def __init__(self, x, y, z, color):\n",
    "        \"\"\"\n",
    "        :param: x: float: x coordinate of the point (in mm)\n",
    "        :param: y: float: y coordinate of the point (in mm)\n",
    "        :param: z: float: z coordinate of the point (in mm)\n",
    "        :param: color: int: integer indicating the color of the point\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "        self.color = color\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"[{self.x}, {self.y}, {self.z}, {self.color}]\"\n",
    "    \n",
    "    def _get_pt_as_array(self):\n",
    "        return np.array([self.x, self.y, self.z])\n",
    "\n",
    "    def get_closest_point_in_cloud(self, pc, filter_by_color=True):\n",
    "\n",
    "        distances = np.array(\n",
    "            [np.linalg.norm(x + y + z) for (x, y, z) in np.abs(pc[:, :3] - self._get_pt_as_array())])\n",
    "\n",
    "        if not filter_by_color:\n",
    "            idx = distances.argmin()\n",
    "            return idx, pc[idx]\n",
    "\n",
    "        if len(np.where(pc[:, 3] == self.color)) == 0:\n",
    "            return None, None\n",
    "\n",
    "        distances[pc[:, 3] != self.color] = np.max(distances) + 1\n",
    "\n",
    "        idx = distances.argmin()\n",
    "\n",
    "        return idx, pc[idx]\n",
    "\n",
    "\n",
    "def extract_spine_id(filename):\n",
    "    \"\"\"\n",
    "    Given a file, it extracts the id of the spine.\n",
    "\n",
    "    Example 1.\n",
    "\n",
    "    .. code-block:: console\n",
    "    >> filename = <spine_folder>\\sspine1_vert1_1_0.txt\n",
    "    >> extract_spine_id(filename)\n",
    "    spine_1\n",
    "\n",
    "    Example 2:\n",
    "    >> filename = spine1_vert1_0.txt\n",
    "    >> extract_spine_id(filename)\n",
    "    spine_1\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    filename = os.path.split(filename)[-1]\n",
    "\n",
    "    return filename.split(\"_\")[0]\n",
    "\n",
    "\n",
    "def indexes2points(idxes_list, point_cloud, color=0):\n",
    "    \"\"\"\n",
    "    Converts a list of indexes or a index to a set of Points, extracting the point coordinates from the input\n",
    "    point_cloud\n",
    "\n",
    "    :param: idxes_list: list(int): list of input indexes\n",
    "    :param: point_cloud: np.ndarray of size [Nx3] or [Nx4]. If the array size is [Nx4], the last dimension is considered\n",
    "        to be the color of the point\n",
    "    :param: color: if the array has size [Nx3], the the color of all the points in the point cloud is set to color\n",
    "        (Default to 0).\n",
    "    :return: a list of Point objects, containing the 3d coordinates and color of the input point cloud at the input\n",
    "        indexes\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: console\n",
    "            >> idxes_list = [1, 3]\n",
    "            >> point_cloud = np.ndarray([ 10, 14, 20, 1\n",
    "                                        [ 10, 30, 20, 4],\n",
    "                                        [ 18, 20, 18, 2],\n",
    "                                        [40, 1, 20, 2])\n",
    "\n",
    "            >> indexes2points(idxes_list, point_cloud)\n",
    "            [Point(x = 10, y = 30, z = 4, color = 1), Point(x = 40, y=1, z = 20, color = 2)]\n",
    "    \"\"\"\n",
    "\n",
    "    if point_cloud.shape[1] > 3:\n",
    "        color = point_cloud[:, 3]\n",
    "    else:\n",
    "        color = np.ones([point_cloud.shape[0],])*color\n",
    "\n",
    "    if isinstance(idxes_list, int) or isinstance(idxes_list, float):\n",
    "        idxes_list = [idxes_list]\n",
    "\n",
    "    constraints_points = []\n",
    "    for item in idxes_list:\n",
    "        if isinstance(item, tuple) or isinstance(item, list):\n",
    "            assert all(isinstance(x, int) for x in item) or all(isinstance(x, float) for x in item)\n",
    "\n",
    "            constraints_points.append(tuple(Point(x=point_cloud[idx, 0],\n",
    "                                                  y=point_cloud[idx, 1],\n",
    "                                                  z=point_cloud[idx, 2],\n",
    "                                                  color=color[idx]) for idx in item))\n",
    "\n",
    "        else:\n",
    "            constraints_points.append(Point(x=point_cloud[item, 0],\n",
    "                                             y=point_cloud[item, 1],\n",
    "                                             z=point_cloud[item, 2],\n",
    "                                             color=color[item]))\n",
    "\n",
    "    if len(constraints_points) == 1:\n",
    "        return constraints_points[0]\n",
    "\n",
    "    return constraints_points\n",
    "\n",
    "\n",
    "def points2indexes(point_list, point_cloud):\n",
    "    \"\"\"\n",
    "    Converts a list of indexes or a points to a set of indexes, corresponding to indexes of the closest points in the\n",
    "    source point cloud.\n",
    "\n",
    "    :param: point_list: list(Point): list of input Point\n",
    "    :param: point_cloud: np.ndarray of size [Nx3]. If the number of columns is higher than 3 (e.g. the input array\n",
    "        has size [Nx4], then only the first 3 columns are considered)\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: console\n",
    "            >> idxes_list = [Point(x = 11, y = 29, z = 3, color = 1), Point(x = 41, y=1, z = 20, color = 2)]\n",
    "            >> point_cloud = np.ndarray([ 10, 14, 20, 1\n",
    "                                        [ 10, 30, 20, 4],\n",
    "                                        [ 18, 20, 18, 2],\n",
    "                                        [40, 1, 20, 2])\n",
    "\n",
    "            >> indexes2points(idxes_list, point_cloud)\n",
    "            [1, 3]\n",
    "    \"\"\"\n",
    "\n",
    "    idxes_list = []\n",
    "\n",
    "    for item in point_list:\n",
    "        if isinstance(item, tuple) or isinstance(item, list):\n",
    "            assert all(isinstance(x, Point) for x in item)\n",
    "            idxes_list.append(tuple(p.get_closest_point_in_cloud(point_cloud)[0] for p in item))\n",
    "\n",
    "        else:\n",
    "            idxes_list.append(item.get_closest_point_in_cloud(point_cloud[0]))\n",
    "\n",
    "    return idxes_list\n",
    "\n",
    "def obtain_indices_raycasted_original_pc(spine_target, r_target):\n",
    "    \"\"\"\n",
    "    Find indices in spine_target w.r.t. r_target such that they are the closest points between the two\n",
    "    point clouds\n",
    "\n",
    "    :param: spine_target: np.ndarray with size [Nx3] with the point cloud for which the closest point indexes are\n",
    "        extracted If the second dimension is higher then 3, only the first 3 dimensions are considered\n",
    "    :param: r_target: np.ndarray with size [Nx3] with the point cloud used to find the closest points in spine_target.\n",
    "        If the second dimension is higher then 3, only the first 3 dimensions are considered\n",
    "\n",
    "    Example:\n",
    "\n",
    "        .. code-block:: console\n",
    "            >> spine_target = np.ndarray([ 10, 14, 20, 1\n",
    "                                [ 10, 30, 20, 4],\n",
    "                                [ 18, 20, 18, 2],\n",
    "                                [40, 1, 20, 2])\n",
    "\n",
    "            >> r_target = np.ndarray([ 18, 21, 18, 2],\n",
    "                                     [40, 1, 20, 3])\n",
    "\n",
    "            >> obtain_indices_raycasted_original_pc(spine_target, r_target)\n",
    "            [2, 3]\n",
    "    \"\"\"\n",
    "    kdtree=KDTree(spine_target[:,:3])\n",
    "    dist,points=kdtree.query(r_target[:,:3],1)\n",
    "\n",
    "    return list(set(points))\n",
    "\n",
    "def create_source_target_with_vertebra_label(source_pc, target_pc, vert):\n",
    "    \"\"\"\n",
    "    source_pc: source point cloud\n",
    "    target_pc: target point cloud\n",
    "    vert: [1-5] for [L1-L5] vertebra respectively\n",
    "    \n",
    "    this function is to create source and target point clouds with label for each vertebra\n",
    "    \"\"\"\n",
    "    \n",
    "    source = np.ones((source_pc.shape[0], source_pc.shape[1]+1))\n",
    "    source[:, :3]=source_pc\n",
    "    source[:, 3] = source[:, 3]*vert\n",
    "    target = np.ones((target_pc.shape[0], target_pc.shape[1]+1))\n",
    "    target[:, :3]=target_pc\n",
    "    target[:, 3]= target[:, 3]*vert\n",
    "    \n",
    "    return source, target\n",
    "\n",
    "def create_source_target_flow_spine(source_pc, target_pc, vert):\n",
    "    \"\"\"\n",
    "    source_pc: source point cloud\n",
    "    target_pc: target point cloud\n",
    "    vert: [1-5] for [L1-L5] vertebra respectively\n",
    "    \n",
    "    this function is to create source and target point clouds with 7D\n",
    "    where the point clouds are centered.\n",
    "    \"\"\"\n",
    "    \n",
    "    source_pc, target_pc = create_source_target_with_vertebra_label(source_pc, target_pc, vert)\n",
    "\n",
    "    centroid_source = centeroidnp(source_pc)\n",
    "    centroid_target = centeroidnp(target_pc)\n",
    "    \n",
    "    source_7d = create_7D(source_pc, centroid_source, centroid_target)\n",
    "    target_7d = create_7D(target_pc, centroid_source, centroid_target)\n",
    "    \n",
    "    flow = target_7d[:,:3]-source_7d[:,:3]\n",
    "    \n",
    "    return source_7d, target_7d, flow\n",
    "\n",
    "\n",
    "def get_lumbar_vertebrae_dict(folder_path):\n",
    "    \"\"\"\n",
    "    Given a timestamp folder, containing the 5 .txt files corresponding to the lumbar vertebrae, the function loads\n",
    "    the vertebra and returns a dict containing the point clouds.\n",
    "    Example, given the folder TestDataOrderingJane\\txt_files\\spine1\\ts_0_0 containing the files (spine1_vert1_0.txt,\n",
    "    spine1_vert2_0.txt, spine1_vert3_0.txt, spine1_vert4_0.txt, spine1_vert5_0.txt), the function returns a dict like\n",
    "    {\"vert1\" : np.array(..), \"vert2\" : np.array(..), \"vert3\" : np.array(..), \"vert4\" : np.array(..),\n",
    "    \"vert5\" : np.array(..)}, where the np.arrays are Nx3 arrays containing the 3D coordinates of the point clouds\n",
    "    of each vertebra\n",
    "\n",
    "    :param folder_path: str: The path to the folder containing the vertebra point clouds .txt files\n",
    "    \"\"\"\n",
    "\n",
    "    vertebra_files = [item for item in os.listdir(folder_path) if \"vert\" in item]\n",
    "\n",
    "    vertebrae_dict = dict()\n",
    "    for vertebra in [\"vert1\", \"vert2\", \"vert3\", \"vert4\", \"vert5\"]:\n",
    "\n",
    "        vert_file = [item for item in vertebra_files if vertebra in item]\n",
    "        assert len(vert_file) == 1\n",
    "\n",
    "        vertebrae_dict[vertebra] = np.loadtxt(os.path.join(folder_path, vert_file[0]))\n",
    "\n",
    "    return vertebrae_dict\n",
    "\n",
    "def load_biomechanical_constraints(constraint_path, spine_id, source_vertebrae_dict):\n",
    "    \"\"\"\n",
    "    Loads the biomechanical constraints and returns them as a list of tuples like\n",
    "    [(c1_1, c1_2), (c2_1, c2_2), ..., (cn_1, cn_2)] where each tuple contains the Point objects\n",
    "     of the \"starting\" point connected to the spring and the index of the \"ending\" point connected to the spring:\n",
    "\n",
    "    ci_1 _/\\/\\/\\/\\_ ci_2\n",
    "\n",
    "    :param: spine_folder_path: str: The path containing the data for a given spine dataset, where the text file\n",
    "        containing the biomechanical constraints is stored\n",
    "    :param: source_vertebrae_dict: The vertebrae dict containing the point cloud corresponding to each vertebra\n",
    "        like:\n",
    "        source_vertebrae_dict = {\"vert1\" : np.array(..), \"vert2\" : np.array(..), \"vert3\" : np.array(..),\n",
    "            \"vert4\" : np.array(..), \"vert5\" : np.array(..)}\n",
    "    \"\"\"\n",
    "\n",
    "    biomechanical_constraints_path = os.path.join(constraint_path,\n",
    "                                                  spine_id.replace(\"s\", \"S\")\n",
    "                                                  + \"_biomechanical.txt\")\n",
    "\n",
    "    print(biomechanical_constraints_path)\n",
    "    if not os.path.exists(biomechanical_constraints_path):\n",
    "        raise ValueError(\"\")\n",
    "\n",
    "    # The biomechanical constraints are saved in an array on a single row, like:\n",
    "    # idx_c1_1, idx_c1_2, idx_c2_1, idx_c2_2, ..., idx_cn_1, idx_cn_2\n",
    "#     biomechanical_constraints_array = np.squeeze(np.loadtxt(biomechanical_constraints_path))\n",
    "#     biomechanical_constraint_list = []\n",
    "#     for i in range(0, biomechanical_constraints_array.shape[0] - 1, 2):\n",
    "#         biomechanical_constraint_list.append(\n",
    "#             (int(biomechanical_constraints_array[i]), int(biomechanical_constraints_array[i + 1]) ))\n",
    "    \n",
    "    \n",
    "    # The biomechanical constraints are saved in an n x 2 on a rows, like:\n",
    "    # idx_c1_1, idx_c1_2, \n",
    "    # idx_c2_1, idx_c2_2, \n",
    "    # ..., \n",
    "    # idx_cn_1, idx_cn_2\n",
    "    biomechanical_constraints_array = np.loadtxt(biomechanical_constraints_path)\n",
    "    biomechanical_constraints_array = np.array(\n",
    "        [item for sublist in biomechanical_constraints_array for item in sublist])\n",
    "\n",
    "    biomechanical_constraint_list = []\n",
    "    for i in range(0, biomechanical_constraints_array.shape[0] - 1, 2):\n",
    "        biomechanical_constraint_list.append(\n",
    "            (int(biomechanical_constraints_array[i]), int(biomechanical_constraints_array[i + 1]) ))\n",
    "        \n",
    "    ######################\n",
    "    \n",
    "    constraints_points = []\n",
    "    dict_keys = [item for item in source_vertebrae_dict.keys()]\n",
    "\n",
    "    for i in range(0, biomechanical_constraints_array.shape[0] - 1, 2):\n",
    "\n",
    "        vert_name = dict_keys[int(i/2)]\n",
    "        next_vert_name = dict_keys[int(i/2) + 1]\n",
    "        p1 = indexes2points(int(biomechanical_constraints_array[i]),\n",
    "                            point_cloud=source_vertebrae_dict[vert_name],\n",
    "                            color=int(i/2) + 1)\n",
    "\n",
    "        p2 = indexes2points(int(biomechanical_constraints_array[i+1]),\n",
    "                            point_cloud=source_vertebrae_dict[next_vert_name],\n",
    "                            color=int(i/2) + 2)\n",
    "\n",
    "        constraints_points.append((p1, p2))\n",
    "\n",
    "    return constraints_points\n",
    "\n",
    "\n",
    "def preprocess_spine_data(spine_path, constraint_path=None):\n",
    "    \"\"\"\n",
    "    Preprocess the data for a given spine dataset. Specifically, for the given spine (i.e. for a given spine_id),\n",
    "    it ierates over all the timestamps for that given spine.\n",
    "    The function does the following.\n",
    "    1. It loads the \"ts0\" as the timestamp of the underformed spine, and therefore of the source spine.\n",
    "    2. It loads the biomechanical constraints for the given spine\n",
    "    3. It iterates over all the timestamps different from t0, where the spine is considered to be deformed compared to\n",
    "        t0, and for each timestamp different from ts0:\n",
    "        3.a Computes the flow from the source to the target points, assuming a correspondence\n",
    "            between points at different timestamps\n",
    "        3.b Concatenates all the vertebrae together for both source (ts0) and target (considered timestamp),\n",
    "            indicating the vertebral level in the resulting concatenated point clouds in a 4th column,\n",
    "            where L1 is indicated with 1, L2 with 2, L3 with 3,\n",
    "            L4 with 4, L5 with 5.\n",
    "        3.c. For each given source-deformed spine pair, generate a Data dict with the following keys:\n",
    "            \"spine_id\", \"source_ts_id\", \"target_ts_id\", \"source_pc\", \"target_pc\", \"flow\", \"biomechanical_constraint\"\n",
    "\n",
    "    \"\"\"\n",
    "    constraint_path = spine_path if constraint_path is None else constraint_path\n",
    "    spine_id = os.path.split(spine_path)[-1]\n",
    "\n",
    "    # Get the folder containing the data relative to the un-deformed spine (source) and the list of folders\n",
    "    # containing the deformed spine\n",
    "    source_timestamp = \"ts0\"\n",
    "    deformed_timestamps = [item for item in os.listdir(spine_path) if item != source_timestamp and \"ts\" in item]\n",
    "\n",
    "    # Getting the source vertebrae dict, as {\"vert1\" : np.array(..), \"vert2\" : np.array(..),\n",
    "    # \"vert3\" : np.array(..), \"vert4\" : np.array(..), \"vert5\" : np.array(..)}\n",
    "    source_vertebrae = get_lumbar_vertebrae_dict(os.path.join(spine_path, source_timestamp))\n",
    "\n",
    "    # Load the biomechanical constraints for the selected spine. biomechanical_constraints is loaded as a list\n",
    "    # of tuples (Point, Point). biomechanical_constraints = [(Point, Point), (Point, Point), ..., (Point, Point)]\n",
    "    # For a given tuple, the first element is the point from which the spring starts, the second point is the point\n",
    "    # where the spring ends. Note that the biomechanical_constraints contain tuple defining the 3D position of the\n",
    "    # constraints, and not their indexes.\n",
    "    biomechanical_constraints = load_biomechanical_constraints(constraint_path=constraint_path,\n",
    "                                                               spine_id=extract_spine_id(spine_path),\n",
    "                                                               source_vertebrae_dict=source_vertebrae)\n",
    "\n",
    "\n",
    "    # Iterate over all the deformed versions (folders) of the source spine and generate the data list\n",
    "    data = []\n",
    "    for deformed_timestamp in deformed_timestamps:\n",
    "\n",
    "        # Getting the target vertebrae dict, as {\"vert1\" : np.array(..), \"vert2\" : np.array(..),\n",
    "        # \"vert3\" : np.array(..), \"vert4\" : np.array(..), \"vert5\" : np.array(..)}\n",
    "        deformed_vertebrae = get_lumbar_vertebrae_dict(os.path.join(spine_path, deformed_timestamp))\n",
    "\n",
    "        # Preprocess the point clouds of each given vertebra and then concatenate the vertebrae in a single point cloud\n",
    "        preprocessed_source_vertebrae = []\n",
    "        preprocessed_target_vertebrae = []\n",
    "        for i, vertebra in enumerate([\"vert1\", \"vert2\", \"vert3\", \"vert4\", \"vert5\"]):\n",
    "            preprocessed_source_pc, preprocessed_target_pc = \\\n",
    "                create_source_target_with_vertebra_label(source_pc=source_vertebrae[vertebra],\n",
    "                                                         target_pc=deformed_vertebrae[vertebra],\n",
    "                                                         vert=i + 1)\n",
    "            preprocessed_source_vertebrae.append(preprocessed_source_pc)\n",
    "            preprocessed_target_vertebrae.append(preprocessed_target_pc)\n",
    "\n",
    "        # Concatenating source and target vertebrae into a single spine point cloud\n",
    "        preprocessed_source_spine = np.concatenate(preprocessed_source_vertebrae)\n",
    "        preprocessed_target_spine = np.concatenate(preprocessed_target_vertebrae)\n",
    "\n",
    "        # Append the generated source-target pair to the data list\n",
    "        data.append({\n",
    "            \"spine_id\": spine_id,\n",
    "            \"source_ts_id\": source_timestamp,\n",
    "            \"target_ts_id\": deformed_timestamp,\n",
    "            \"source_pc\": preprocessed_source_spine,\n",
    "            \"target_pc\": preprocessed_target_spine,\n",
    "            \"flow\": preprocessed_target_spine[:, :3] - preprocessed_source_spine[:, :3],\n",
    "            \"biomechanical_constraint\": biomechanical_constraints\n",
    "        })\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_ray_casted_data(data, raycasted_txt_path):\n",
    "    # Loading the raycasted point clouds\n",
    "    source_ray_casted_pc = np.loadtxt(os.path.join(raycasted_txt_path, data[\"spine_id\"],\n",
    "                                                   \"raycasted_\" + data[\"source_ts_id\"] + \".txt\"))\n",
    "    target_ray_casted_pc = np.loadtxt(os.path.join(raycasted_txt_path, data[\"spine_id\"],\n",
    "                                                   \"raycasted_\" + data[\"target_ts_id\"] + \".txt\"))\n",
    "\n",
    "    # Getting the flow at the biomechanical_constraints points as it will be needed later\n",
    "    constraint_indexes = points2indexes(point_list=data[\"biomechanical_constraint\"],\n",
    "                                        point_cloud=data[\"source_pc\"])\n",
    "\n",
    "    constraint_points, constraint_flows = [], []\n",
    "    for (p1_idx, p2_idx) in constraint_indexes:\n",
    "        p1_colored, p2_colored = data[\"source_pc\"][p1_idx, :], data[\"source_pc\"][p2_idx, :]\n",
    "        p1_flow, p2_flow = data[\"flow\"][p1_idx, :], data[\"flow\"][p2_idx, :]\n",
    "\n",
    "        constraint_points.append((p1_colored, p2_colored))\n",
    "        constraint_flows.append((p1_flow, p2_flow))\n",
    "\n",
    "    # Getting the indexes of the points in the source data which are closest to the ray_casted source points\n",
    "    source_ray_casted_idxes = obtain_indices_raycasted_original_pc(spine_target=data[\"source_pc\"],\n",
    "                                                                   r_target=source_ray_casted_pc)\n",
    "    data[\"source_pc\"] = data[\"source_pc\"][source_ray_casted_idxes]\n",
    "    data[\"flow\"] = data[\"flow\"][source_ray_casted_idxes]\n",
    "\n",
    "    # Getting the indexes of the points in the target data which are closest to the ray_casted target points\n",
    "    target_ray_casted_idxes = obtain_indices_raycasted_original_pc(spine_target=data[\"target_pc\"],\n",
    "                                                                   r_target=target_ray_casted_pc)\n",
    "    #data[\"target_pc\"] = data[\"target_pc\"][target_ray_casted_idxes]\n",
    "    data[\"target_pc\"]=target_ray_casted_pc\n",
    "\n",
    "    # Adding the biomechanical constraints to the source as they might be not present due to the ray-casting\n",
    "    new_constraints_idx = []\n",
    "    for (p1, p2), (flow1, flow2) in zip(constraint_points, constraint_flows):\n",
    "        data[\"source_pc\"] = np.concatenate((data[\"source_pc\"], np.reshape(p1, [1, 4]), \n",
    "                                            np.reshape(p2, [1, 4])), axis=0)\n",
    "        data[\"flow\"] = np.concatenate((data[\"flow\"], np.reshape(flow1, [1, 3]), \n",
    "                                       np.reshape(flow2, [1, 3])), axis=0)\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_color_code(color_name):\n",
    "    color_code_dict = {\n",
    "        \"dark_green\" : \"0 0.333 0 1\",\n",
    "        \"yellow\": \"1 1 0 1\",\n",
    "        \"default\": \"1 1 0 1\"\n",
    "    }\n",
    "\n",
    "    if color_name in color_code_dict.keys():\n",
    "        return color_code_dict[color_name]\n",
    "\n",
    "    else:\n",
    "        return color_code_dict[\"default\"]\n",
    "\n",
    "def save_for_sanity_check(data, save_dir):\n",
    "    \"\"\"\n",
    "    Saving the generated data in imfusion workspaces at specific location\n",
    "    \"\"\"\n",
    "\n",
    "    source_pc = data[\"source_pc\"][:, :3]\n",
    "    target_pc = data[\"target_pc\"][:, :3]\n",
    "\n",
    "    gt_target_pc = source_pc + data[\"flow\"]\n",
    "\n",
    "    save_folder_path = os.path.join(save_dir, data[\"spine_id\"], data[\"target_ts_id\"])\n",
    "    if not os.path.exists(save_folder_path):\n",
    "        os.makedirs(save_folder_path)\n",
    "\n",
    "    # saving the point clouds\n",
    "    # 1. Saving the full point clouds\n",
    "    np.savetxt(os.path.join(save_folder_path, \"full_source_pc.txt\"), source_pc[:, :3])\n",
    "    np.savetxt(os.path.join(save_folder_path, \"full_target_pc.txt\"), target_pc[:, :3])\n",
    "    np.savetxt(os.path.join(save_folder_path, \"full_gt_pc.txt\"), gt_target_pc[:, :3])\n",
    "\n",
    "    ps_list = [(\"full_source_pc\", os.path.join(save_folder_path, \"full_source_pc.txt\"),\n",
    "                get_color_code(\"dark_green\")),\n",
    "               (\"full_target_pc\", os.path.join(save_folder_path, \"full_target_pc.txt\"), \n",
    "                get_color_code(\"yellow\")),\n",
    "               (\"full_gt_pc\", os.path.join(save_folder_path, \"full_gt_pc.txt\"), \n",
    "                get_color_code(\"yellow\"))]\n",
    "\n",
    "    imf_tree, imf_root = utils.get_empty_imfusion_ws()\n",
    "\n",
    "    for i, (name, path, color) in enumerate(ps_list):\n",
    "\n",
    "        imf_root = utils.add_block_to_xml(imf_root,\n",
    "                                          parent_block_name=\"Annotations\",\n",
    "                                          block_name=\"point_cloud_annotation\",\n",
    "                                          param_dict={\"referenceDataUid\":\"data\" + str(i),\n",
    "                                                      \"name\": str(name),\n",
    "                                                      \"color\": str(color),\n",
    "                                                      \"labelText\":\"some\",\n",
    "                                                      \"pointSize\": \"2\"})\n",
    "\n",
    "        imf_root = utils.add_block_to_xml(imf_root,\n",
    "                                          parent_block_name=\"Algorithms\",\n",
    "                                          block_name=\"load_point_cloud\",\n",
    "                                          param_dict={\"location\": path,\n",
    "                                                      \"outputUids\": \"data\" + str(i)})\n",
    "\n",
    "    # Adding the biomechanical_constraints\n",
    "\n",
    "    for i, (c1, c2) in enumerate(data[\"biomechanical_constraint\"]):\n",
    "\n",
    "        c1_idx, _ = c1.get_closest_point_in_cloud(data[\"source_pc\"], filter_by_color=True)\n",
    "        c2_idx, _ = c2.get_closest_point_in_cloud(data[\"source_pc\"], filter_by_color=True)\n",
    "\n",
    "        p1 = data[\"source_pc\"][c1_idx, :3]\n",
    "        p2 = data[\"source_pc\"][c2_idx, :3]\n",
    "        points = \" \".join([str(item) for item in p1]) + \" \" + \" \".join([str(item) for item in p2])\n",
    "        imf_root = utils.add_block_to_xml(imf_root,\n",
    "                                          parent_block_name=\"Annotations\",\n",
    "                                          block_name=\"segment_annotation\",\n",
    "                                          param_dict={\"name\": \"constraint_\" + str(i+1),\n",
    "                                                      \"points\": points})\n",
    "\n",
    "    utils.write_on_file(imf_tree, os.path.join(save_folder_path, \"imf_ws.iws\"))\n",
    "\n",
    "\n",
    "def generate_npz_files(src_txt_pc_path, dst_npz_path, src_raycasted_pc_path=\"\", ray_casted=False,\n",
    "                       constraint_path = None, dst_sanity_check_data=\"\"):\n",
    "\n",
    "    if not os.path.exists(dst_npz_path):\n",
    "        os.makedirs(dst_npz_path)\n",
    "\n",
    "    # Iterate over all the patients (spine_id) in the dataset\n",
    "    for spine_id in os.listdir(src_txt_pc_path):\n",
    "        if spine_id == \".DS_Store\":\n",
    "            continue\n",
    "\n",
    "        if spine_id == \"spine1\" or spine_id == \"spine10\" or spine_id == \"spine11\" or spine_id == spine_id == \"spine12\" \\\n",
    "                or spine_id == \"spine13\" or spine_id == \"spine14\" or spine_id == \"spine15\":\n",
    "            continue\n",
    "\n",
    "        # Getting the dataset for the specific patient id (spine). It is a list of dict like:\n",
    "        # [{\"source_ts_id\": ts0,\n",
    "        #   \"target_ts_id\": ts_19_0,\n",
    "        #   \"source_pc\": np.ndarray([])\n",
    "        #   \"target_pc\": np.ndarray([])\n",
    "        #   \"biomechanical_constraint\": np.ndarray([])}, ...]\n",
    "        spine_data = preprocess_spine_data(os.path.join(src_txt_pc_path, spine_id), constraint_path=constraint_path)\n",
    "\n",
    "        for data in spine_data:\n",
    "            if ray_casted:\n",
    "                data = get_ray_casted_data(data, src_raycasted_pc_path)\n",
    "                filename = os.path.join(dst_npz_path, \"raycasted_\" + spine_id + \"_\" + data[\"target_ts_id\"] + \".npz\")\n",
    "            else:\n",
    "                filename = os.path.join(dst_npz_path, spine_id + \"_\" + data[\"target_ts_id\"] + \".npz\")\n",
    "\n",
    "            save_for_sanity_check(data, dst_sanity_check_data)\n",
    "\n",
    "            # convert biomechanical_constraint to a 1-d array, putting all the constraint on a single \n",
    "            # row - this needs to be changed in future to be a list of tuple or similar format where it is clear \n",
    "            # which point belongs to the same connecting spring\n",
    "\n",
    "            constraint_indexes = points2indexes(data[\"biomechanical_constraint\"], data[\"source_pc\"])\n",
    "            flattened_constraints = [i for sub in constraint_indexes for i in sub]\n",
    "            np.savez_compressed(file=filename,\n",
    "                                flow=data[\"flow\"],\n",
    "                                pc1=data[\"source_pc\"],\n",
    "                                pc2=data[\"target_pc\"],\n",
    "                                ctsPts=flattened_constraints)\n",
    "\n",
    "generate_npz_files(src_txt_pc_path=\"E:/NAS/jane_project/txt_files\",\n",
    "                   dst_npz_path=\"E:/NAS/jane_project/nas_data/new_data_us2\",\n",
    "                   ray_casted=True,\n",
    "                   src_raycasted_pc_path=\"E:/NAS/jane_project/txt_files_us\",\n",
    "                   constraint_path = \"E:/NAS/jane_project/spine_constraints\",\n",
    "                   dst_sanity_check_data=\"E:/NAS/jane_project/sanity_check\")\n",
    "\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}