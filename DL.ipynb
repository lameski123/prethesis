{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "07ebcb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import os.path\n",
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "import glob\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "path = \"/Users/janelameski/Desktop/jane/Thesis/point_clouds/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e5e5e1",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f290dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list the files in path\n",
    "file_list = []\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_list.append(os.path.join(path, file))\n",
    "        \n",
    "file_list = sorted(file_list)\n",
    "target_files = [a for a in file_list if \"target\" in a]\n",
    "source_files = [a for a in file_list if \"source\" in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c430164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = np.loadtxt(path+\"0A0602_source.txt\")\n",
    "file2 = np.loadtxt(path+\"0A0602_target.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52db4996",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#loader\n",
    "class DatasetLoader():\n",
    "    def __init__(self, root=path, npoints=4096):\n",
    "        self.npoints = npoints\n",
    "        self.root = root\n",
    "        self.sourcepath = sorted(glob.glob(os.path.join(self.root, '*source.txt')))\n",
    "        self.targetpath = sorted(glob.glob(os.path.join(self.root, '*target.txt')))\n",
    "       \n",
    "        self.cache = {}\n",
    "        self.cache_size = 30000\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "#         print(self.getSourceTargetPath(index))\n",
    "        if index in self.cache:\n",
    "            source, target = self.cache[index]\n",
    "        else:\n",
    "            s = self.sourcepath[index]\n",
    "            t = self.targetpath[index]\n",
    "#             print(s, t)\n",
    "            source = np.loadtxt(s)\n",
    "            target = np.loadtxt(t)\n",
    "\n",
    "            if len(self.cache) < self.cache_size:\n",
    "                self.cache[index] = (source,target)\n",
    "\n",
    "\n",
    "        n1 = source.shape[0]\n",
    "        sample_idx1 = np.random.choice(n1, self.npoints, replace=False)\n",
    "        n2 = target.shape[0]\n",
    "        sample_idx2 = np.random.choice(n2, self.npoints, replace=False)\n",
    "\n",
    "        source_ = np.copy(source[sample_idx1, :])\n",
    "        target_ = np.copy(target[sample_idx2, :])\n",
    "\n",
    "        return source_, target_\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sourcepath)\n",
    "    \n",
    "    def getSourceTargetPath(self, index):\n",
    "        return (self.sourcepath[index], self.targetpath[index])\n",
    "        \n",
    "\n",
    "def plot_pcs(pc1, pc2):\n",
    "    print(pc1.shape)\n",
    "    print(pc2.shape)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(pc1[:,0], pc1[:,1], pc1[:,2], c='r', marker='o')\n",
    "    ax.scatter(pc2[:,0], pc2[:,1], pc2[:,2], c='b', marker='^')\n",
    "\n",
    "# d = DatasetLoader()\n",
    "\n",
    "# for i in range(5):\n",
    "#     pc1, pc2, = d[i]\n",
    "#     plot_pcs(pc1,pc2)\n",
    "#     break\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec4cb260",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 4,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6fd8db14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#testing data loader\n",
    "######################################\n",
    "# training_set = DatasetLoader()\n",
    "# training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
    "\n",
    "# for epoch in range(4):\n",
    "#     for source, target in training_generator:\n",
    "#         print(source.shape)\n",
    "#######################################        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80687f89",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf8c861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pc_normalize(pc):\n",
    "    centroid = np.mean(pc, axis=0)\n",
    "    pc = pc - centroid\n",
    "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
    "    pc = pc / m\n",
    "    return pc\n",
    "\n",
    "def translate_pointcloud(pointcloud):\n",
    "    xyz1 = np.random.uniform(low=2. / 3., high=3. / 2., size=[3])\n",
    "    xyz2 = np.random.uniform(low=-0.2, high=0.2, size=[3])\n",
    "\n",
    "    translated_pointcloud = np.add(np.multiply(pointcloud, xyz1), xyz2).astype('float32')\n",
    "    return translated_pointcloud\n",
    "\n",
    "\n",
    "def jitter_pointcloud(pointcloud, sigma=0.01, clip=0.05):\n",
    "    N, C = pointcloud.shape\n",
    "    pointcloud += np.clip(sigma * np.random.randn(N, C), -1 * clip, clip)\n",
    "    return pointcloud\n",
    "\n",
    "def farthest_subsample_points(pointcloud1, pointcloud2, num_subsampled_points=768):\n",
    "    pointcloud1 = pointcloud1.T\n",
    "    pointcloud2 = pointcloud2.T\n",
    "    num_points = pointcloud1.shape[0]\n",
    "    nbrs1 = NearestNeighbors(n_neighbors=num_subsampled_points, algorithm='auto',\n",
    "                             metric=lambda x, y: minkowski(x, y), n_jobs=1).fit(pointcloud1)\n",
    "    random_p1 = np.random.random(size=(1, 3)) + np.array([[500, 500, 500]]) * np.random.choice([1, -1, 1, -1])\n",
    "    idx1 = nbrs1.kneighbors(random_p1, return_distance=False).reshape((num_subsampled_points,))\n",
    "    nbrs2 = NearestNeighbors(n_neighbors=num_subsampled_points, algorithm='auto',\n",
    "                             metric=lambda x, y: minkowski(x, y), n_jobs=1).fit(pointcloud2)\n",
    "    random_p2 = random_p1 #np.random.random(size=(1, 3)) + np.array([[500, 500, 500]]) * np.random.choice([1, -1, 2, -2])\n",
    "    idx2 = nbrs2.kneighbors(random_p2, return_distance=False).reshape((num_subsampled_points,))\n",
    "    return pointcloud1[idx1, :].T, pointcloud2[idx2, :].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c854a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeit(tag, t):\n",
    "    print(\"{}: {}s\".format(tag, time() - t))\n",
    "    return time()\n",
    "\n",
    "def pc_normalize(pc):\n",
    "    l = pc.shape[0]\n",
    "    centroid = np.mean(pc, axis=0)\n",
    "    pc = pc - centroid\n",
    "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
    "    pc = pc / m\n",
    "    return pc\n",
    "\n",
    "def square_distance(src, dst):\n",
    "    \"\"\"\n",
    "    Calculate Euclid distance between each two points.\n",
    "    src^T * dst = xn * xm + yn * ym + zn * zmï¼›\n",
    "    sum(src^2, dim=-1) = xn*xn + yn*yn + zn*zn;\n",
    "    sum(dst^2, dim=-1) = xm*xm + ym*ym + zm*zm;\n",
    "    dist = (xn - xm)^2 + (yn - ym)^2 + (zn - zm)^2\n",
    "         = sum(src**2,dim=-1)+sum(dst**2,dim=-1)-2*src^T*dst\n",
    "    Input:\n",
    "        src: source points, [B, N, C]\n",
    "        dst: target points, [B, M, C]\n",
    "    Output:\n",
    "        dist: per-point square distance, [B, N, M]\n",
    "    \"\"\"\n",
    "    B, N, _ = src.shape\n",
    "    _, M, _ = dst.shape\n",
    "    dist = -2 * torch.matmul(src, dst.permute(0, 2, 1))\n",
    "    dist += torch.sum(src ** 2, -1).view(B, N, 1)\n",
    "    dist += torch.sum(dst ** 2, -1).view(B, 1, M)\n",
    "    return dist\n",
    "\n",
    "\n",
    "def index_points(points, idx):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        points: input points data, [B, N, C]\n",
    "        idx: sample index data, [B, S]\n",
    "    Return:\n",
    "        new_points:, indexed points data, [B, S, C]\n",
    "    \"\"\"\n",
    "    device = points.device\n",
    "    B = points.shape[0]\n",
    "    view_shape = list(idx.shape)\n",
    "    view_shape[1:] = [1] * (len(view_shape) - 1)\n",
    "    repeat_shape = list(idx.shape)\n",
    "    repeat_shape[0] = 1\n",
    "    batch_indices = torch.arange(B, dtype=torch.long).to(device).view(view_shape).repeat(repeat_shape)\n",
    "    new_points = points[batch_indices, idx, :]\n",
    "    return new_points\n",
    "\n",
    "\n",
    "def farthest_point_sample(xyz, npoint):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        xyz: pointcloud data, [B, N, 3]\n",
    "        npoint: number of samples\n",
    "    Return:\n",
    "        centroids: sampled pointcloud index, [B, npoint]\n",
    "    \"\"\"\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    centroids = torch.zeros(B, npoint, dtype=torch.long).to(device)\n",
    "    distance = torch.ones(B, N).to(device) * 1e10\n",
    "    farthest = torch.randint(0, N, (B,), dtype=torch.long).to(device)\n",
    "    batch_indices = torch.arange(B, dtype=torch.long).to(device)\n",
    "    for i in range(npoint):\n",
    "        centroids[:, i] = farthest\n",
    "        centroid = xyz[batch_indices, farthest, :].view(B, 1, 3)\n",
    "        dist = torch.sum((xyz - centroid) ** 2, -1)\n",
    "        mask = dist < distance\n",
    "        distance[mask] = dist[mask]\n",
    "        farthest = torch.max(distance, -1)[1]\n",
    "    return centroids\n",
    "\n",
    "\n",
    "def query_ball_point(radius, nsample, xyz, new_xyz):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        radius: local region radius\n",
    "        nsample: max sample number in local region\n",
    "        xyz: all points, [B, N, 3]\n",
    "        new_xyz: query points, [B, S, 3]\n",
    "    Return:\n",
    "        group_idx: grouped points index, [B, S, nsample]\n",
    "    \"\"\"\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    _, S, _ = new_xyz.shape\n",
    "    group_idx = torch.arange(N, dtype=torch.long).to(device).view(1, 1, N).repeat([B, S, 1])\n",
    "    sqrdists = square_distance(new_xyz, xyz)\n",
    "    group_idx[sqrdists > radius ** 2] = N\n",
    "    group_idx = group_idx.sort(dim=-1)[0][:, :, :nsample]\n",
    "    group_first = group_idx[:, :, 0].view(B, S, 1).repeat([1, 1, nsample])\n",
    "    mask = group_idx == N\n",
    "    group_idx[mask] = group_first[mask]\n",
    "    return group_idx\n",
    "\n",
    "def knn_point(k, pos1, pos2):\n",
    "    '''\n",
    "    Input:\n",
    "        k: int32, number of k in k-nn search\n",
    "        pos1: (batch_size, ndataset, c) float32 array, input points\n",
    "        pos2: (batch_size, npoint, c) float32 array, query points\n",
    "    Output:\n",
    "        val: (batch_size, npoint, k) float32 array, L2 distances\n",
    "        idx: (batch_size, npoint, k) int32 array, indices to input points\n",
    "    '''\n",
    "    B, N, C = pos1.shape\n",
    "    M = pos2.shape[1]\n",
    "    pos1 = pos1.view(B,1,N,-1).repeat(1,M,1,1)\n",
    "    pos2 = pos2.view(B,M,1,-1).repeat(1,1,N,1)\n",
    "    dist = torch.sum(-(pos1-pos2)**2,-1)\n",
    "    val,idx = dist.topk(k=k,dim = -1)\n",
    "    return torch.sqrt(-val), idx\n",
    "\n",
    "\n",
    "def sample_and_group(npoint, radius, nsample, xyz, points, returnfps=False):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        npoint:\n",
    "        radius:\n",
    "        nsample:\n",
    "        xyz: input points position data, [B, N, 3]\n",
    "        points: input points data, [B, N, D]\n",
    "    Return:\n",
    "        new_xyz: sampled points position data, [B, npoint, nsample, 3]\n",
    "        new_points: sampled points data, [B, npoint, nsample, 3+D]\n",
    "    \"\"\"\n",
    "    B, N, C = xyz.shape\n",
    "    S = npoint\n",
    "    fps_idx = farthest_point_sample(xyz, npoint) # [B, npoint, C]\n",
    "    new_xyz = index_points(xyz, fps_idx)\n",
    "    idx = query_ball_point(radius, nsample, xyz, new_xyz)\n",
    "    grouped_xyz = index_points(xyz, idx) # [B, npoint, nsample, C]\n",
    "    grouped_xyz_norm = grouped_xyz - new_xyz.view(B, S, 1, C)\n",
    "\n",
    "    if points is not None:\n",
    "        grouped_points = index_points(points, idx)\n",
    "        new_points = torch.cat([grouped_xyz_norm, grouped_points], dim=-1) # [B, npoint, nsample, C+D]\n",
    "    else:\n",
    "        new_points = grouped_xyz_norm\n",
    "    if returnfps:\n",
    "        return new_xyz, new_points, grouped_xyz, fps_idx\n",
    "    else:\n",
    "        return new_xyz, new_points\n",
    "\n",
    "\n",
    "def sample_and_group_all(xyz, points):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        xyz: input points position data, [B, N, 3]\n",
    "        points: input points data, [B, N, D]\n",
    "    Return:\n",
    "        new_xyz: sampled points position data, [B, 1, 3]\n",
    "        new_points: sampled points data, [B, 1, N, 3+D]\n",
    "    \"\"\"\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    new_xyz = torch.zeros(B, 1, C).to(device)\n",
    "    grouped_xyz = xyz.view(B, 1, N, C)\n",
    "    if points is not None:\n",
    "        new_points = torch.cat([grouped_xyz, points.view(B, 1, N, -1)], dim=-1)\n",
    "    else:\n",
    "        new_points = grouped_xyz\n",
    "    return new_xyz, new_points\n",
    "\n",
    "\n",
    "class PointNetSetAbstraction(nn.Module):\n",
    "    def __init__(self, npoint, radius, nsample, in_channel, mlp, group_all):\n",
    "        super(PointNetSetAbstraction, self).__init__()\n",
    "        self.npoint = npoint\n",
    "        self.radius = radius\n",
    "        self.nsample = nsample\n",
    "        self.mlp_convs = nn.ModuleList()\n",
    "        self.mlp_bns = nn.ModuleList()\n",
    "        last_channel = in_channel\n",
    "        for out_channel in mlp:\n",
    "            self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1))\n",
    "            self.mlp_bns.append(nn.BatchNorm2d(out_channel))\n",
    "            last_channel = out_channel\n",
    "        self.group_all = group_all\n",
    "\n",
    "    def forward(self, xyz, points):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            xyz: input points position data, [B, C, N]\n",
    "            points: input points data, [B, D, N]\n",
    "        Return:\n",
    "            new_xyz: sampled points position data, [B, C, S]\n",
    "            new_points_concat: sample points feature data, [B, D', S]\n",
    "        \"\"\"\n",
    "        xyz = xyz.permute(0, 2, 1)\n",
    "        if points is not None:\n",
    "            points = points.permute(0, 2, 1)\n",
    "\n",
    "        if self.group_all:\n",
    "            new_xyz, new_points = sample_and_group_all(xyz, points)\n",
    "        else:\n",
    "            new_xyz, new_points = sample_and_group(self.npoint, self.radius, self.nsample, xyz, points)\n",
    "        # new_xyz: sampled points position data, [B, npoint, C]\n",
    "        # new_points: sampled points data, [B, npoint, nsample, C+D]\n",
    "        new_points = new_points.permute(0, 3, 2, 1) # [B, C+D, nsample,npoint]\n",
    "        for i, conv in enumerate(self.mlp_convs):\n",
    "            bn = self.mlp_bns[i]\n",
    "            new_points =  F.relu(bn(conv(new_points)), inplace=True)\n",
    "\n",
    "        new_points = torch.max(new_points, 2)[0]\n",
    "        new_xyz = new_xyz.permute(0, 2, 1)\n",
    "        return new_xyz, new_points\n",
    "\n",
    "\n",
    "class PointNetSetAbstractionMsg(nn.Module):\n",
    "    def __init__(self, npoint, radius_list, nsample_list, in_channel, mlp_list):\n",
    "        super(PointNetSetAbstractionMsg, self).__init__()\n",
    "        self.npoint = npoint\n",
    "        self.radius_list = radius_list\n",
    "        self.nsample_list = nsample_list\n",
    "        self.conv_blocks = nn.ModuleList()\n",
    "        self.bn_blocks = nn.ModuleList()\n",
    "        for i in range(len(mlp_list)):\n",
    "            convs = nn.ModuleList()\n",
    "            bns = nn.ModuleList()\n",
    "            last_channel = in_channel + 3\n",
    "            for out_channel in mlp_list[i]:\n",
    "                convs.append(nn.Conv2d(last_channel, out_channel, 1))\n",
    "                bns.append(nn.BatchNorm2d(out_channel))\n",
    "                last_channel = out_channel\n",
    "            self.conv_blocks.append(convs)\n",
    "            self.bn_blocks.append(bns)\n",
    "\n",
    "    def forward(self, xyz, points):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            xyz: input points position data, [B, C, N]\n",
    "            points: input points data, [B, D, N]\n",
    "        Return:\n",
    "            new_xyz: sampled points position data, [B, C, S]\n",
    "            new_points_concat: sample points feature data, [B, D', S]\n",
    "        \"\"\"\n",
    "        xyz = xyz.permute(0, 2, 1)\n",
    "        if points is not None:\n",
    "            points = points.permute(0, 2, 1)\n",
    "\n",
    "        B, N, C = xyz.shape\n",
    "        S = self.npoint\n",
    "        new_xyz = index_points(xyz, farthest_point_sample(xyz, S))\n",
    "        new_points_list = []\n",
    "        for i, radius in enumerate(self.radius_list):\n",
    "            K = self.nsample_list[i]\n",
    "            group_idx = query_ball_point(radius, K, xyz, new_xyz)\n",
    "            grouped_xyz = index_points(xyz, group_idx)\n",
    "            grouped_xyz -= new_xyz.view(B, S, 1, C)\n",
    "            if points is not None:\n",
    "                grouped_points = index_points(points, group_idx)\n",
    "                grouped_points = torch.cat([grouped_points, grouped_xyz], dim=-1)\n",
    "            else:\n",
    "                grouped_points = grouped_xyz\n",
    "\n",
    "            grouped_points = grouped_points.permute(0, 3, 2, 1)  # [B, D, K, S]\n",
    "            for j in range(len(self.conv_blocks[i])):\n",
    "                conv = self.conv_blocks[i][j]\n",
    "                bn = self.bn_blocks[i][j]\n",
    "                grouped_points =  F.relu(bn(conv(grouped_points)), inplace=True)\n",
    "            new_points = torch.max(grouped_points, 2)[0]  # [B, D', S]\n",
    "            new_points_list.append(new_points)\n",
    "\n",
    "        new_xyz = new_xyz.permute(0, 2, 1)\n",
    "        new_points_concat = torch.cat(new_points_list, dim=1)\n",
    "        return new_xyz, new_points_concat\n",
    "\n",
    "\n",
    "class PointNetFeaturePropagation(nn.Module):\n",
    "    def __init__(self, in_channel, mlp):\n",
    "        super(PointNetFeaturePropagation, self).__init__()\n",
    "        self.mlp_convs = nn.ModuleList()\n",
    "        self.mlp_bns = nn.ModuleList()\n",
    "        last_channel = in_channel\n",
    "        for out_channel in mlp:\n",
    "            self.mlp_convs.append(nn.Conv1d(last_channel, out_channel, 1))\n",
    "            self.mlp_bns.append(nn.BatchNorm1d(out_channel))\n",
    "            last_channel = out_channel\n",
    "\n",
    "    def forward(self, xyz1, xyz2, points1, points2):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            xyz1: input points position data, [B, C, N]\n",
    "            xyz2: sampled input points position data, [B, C, S]\n",
    "            points1: input points data, [B, D, N]\n",
    "            points2: input points data, [B, D, S]\n",
    "        Return:\n",
    "            new_points: upsampled points data, [B, D', N]\n",
    "        \"\"\"\n",
    "        xyz1 = xyz1.permute(0, 2, 1)\n",
    "        xyz2 = xyz2.permute(0, 2, 1)\n",
    "\n",
    "        points2 = points2.permute(0, 2, 1)\n",
    "        B, N, C = xyz1.shape\n",
    "        _, S, _ = xyz2.shape\n",
    "\n",
    "        if S == 1:\n",
    "            interpolated_points = points2.repeat(1, N, 1)\n",
    "        else:\n",
    "            dists = square_distance(xyz1, xyz2)\n",
    "            dists, idx = dists.sort(dim=-1)\n",
    "            dists, idx = dists[:, :, :3], idx[:, :, :3]  # [B, N, 3]\n",
    "\n",
    "            dist_recip = 1.0 / (dists + 1e-8)\n",
    "            norm = torch.sum(dist_recip, dim=2, keepdim=True)\n",
    "            weight = dist_recip / norm\n",
    "            interpolated_points = torch.sum(index_points(points2, idx) * weight.view(B, N, 3, 1), dim=2)\n",
    "\n",
    "        if points1 is not None:\n",
    "            points1 = points1.permute(0, 2, 1)\n",
    "            new_points = torch.cat([points1, interpolated_points], dim=-1)\n",
    "        else:\n",
    "            new_points = interpolated_points\n",
    "\n",
    "        new_points = new_points.permute(0, 2, 1)\n",
    "        for i, conv in enumerate(self.mlp_convs):\n",
    "            bn = self.mlp_bns[i]\n",
    "            new_points = F.relu(bn(conv(new_points)), inplace=True)\n",
    "        return new_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6231b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model utils\n",
    "\n",
    "\n",
    "class SharedMLP(nn.Sequential):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            args: List[int],\n",
    "            *,\n",
    "            bn: bool = False,\n",
    "            activation=nn.ReLU(inplace=True),\n",
    "            preact: bool = False,\n",
    "            first: bool = False,\n",
    "            name: str = \"\",\n",
    "            instance_norm: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        for i in range(len(args) - 1):\n",
    "            self.add_module(\n",
    "                name + 'layer{}'.format(i),\n",
    "                Conv2d(\n",
    "                    args[i],\n",
    "                    args[i + 1],\n",
    "                    bn=(not first or not preact or (i != 0)) and bn,\n",
    "                    activation=activation\n",
    "                    if (not first or not preact or (i != 0)) else None,\n",
    "                    preact=preact,\n",
    "                    instance_norm=instance_norm\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "class _ConvBase(nn.Sequential):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_size,\n",
    "            out_size,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            activation,\n",
    "            bn,\n",
    "            init,\n",
    "            conv=None,\n",
    "            batch_norm=None,\n",
    "            bias=True,\n",
    "            preact=False,\n",
    "            name=\"\",\n",
    "            instance_norm=False,\n",
    "            instance_norm_func=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        bias = bias and (not bn)\n",
    "        conv_unit = conv(\n",
    "            in_size,\n",
    "            out_size,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=bias\n",
    "        )\n",
    "        init(conv_unit.weight)\n",
    "        if bias:\n",
    "            nn.init.constant_(conv_unit.bias, 0)\n",
    "\n",
    "        if bn:\n",
    "            if not preact:\n",
    "                bn_unit = batch_norm(out_size)\n",
    "            else:\n",
    "                bn_unit = batch_norm(in_size)\n",
    "        if instance_norm:\n",
    "            if not preact:\n",
    "                in_unit = instance_norm_func(out_size, affine=False, track_running_stats=False)\n",
    "            else:\n",
    "                in_unit = instance_norm_func(in_size, affine=False, track_running_stats=False)\n",
    "\n",
    "        if preact:\n",
    "            if bn:\n",
    "                self.add_module(name + 'bn', bn_unit)\n",
    "\n",
    "            if activation is not None:\n",
    "                self.add_module(name + 'activation', activation)\n",
    "\n",
    "            if not bn and instance_norm:\n",
    "                self.add_module(name + 'in', in_unit)\n",
    "\n",
    "        self.add_module(name + 'conv', conv_unit)\n",
    "\n",
    "        if not preact:\n",
    "            if bn:\n",
    "                self.add_module(name + 'bn', bn_unit)\n",
    "\n",
    "            if activation is not None:\n",
    "                self.add_module(name + 'activation', activation)\n",
    "\n",
    "            if not bn and instance_norm:\n",
    "                self.add_module(name + 'in', in_unit)\n",
    "\n",
    "\n",
    "class _BNBase(nn.Sequential):\n",
    "\n",
    "    def __init__(self, in_size, batch_norm=None, name=\"\"):\n",
    "        super().__init__()\n",
    "        self.add_module(name + \"bn\", batch_norm(in_size))\n",
    "\n",
    "        nn.init.constant_(self[0].weight, 1.0)\n",
    "        nn.init.constant_(self[0].bias, 0)\n",
    "\n",
    "\n",
    "class BatchNorm1d(_BNBase):\n",
    "\n",
    "    def __init__(self, in_size: int, *, name: str = \"\"):\n",
    "        super().__init__(in_size, batch_norm=nn.BatchNorm1d, name=name)\n",
    "\n",
    "\n",
    "class BatchNorm2d(_BNBase):\n",
    "\n",
    "    def __init__(self, in_size: int, name: str = \"\"):\n",
    "        super().__init__(in_size, batch_norm=nn.BatchNorm2d, name=name)\n",
    "\n",
    "\n",
    "class Conv1d(_ConvBase):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_size: int,\n",
    "            out_size: int,\n",
    "            *,\n",
    "            kernel_size: int = 1,\n",
    "            stride: int = 1,\n",
    "            padding: int = 0,\n",
    "            activation=nn.ReLU(inplace=True),\n",
    "            bn: bool = False,\n",
    "            init=nn.init.kaiming_normal_,\n",
    "            bias: bool = True,\n",
    "            preact: bool = False,\n",
    "            name: str = \"\",\n",
    "            instance_norm=False\n",
    "    ):\n",
    "        super().__init__(\n",
    "            in_size,\n",
    "            out_size,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            activation,\n",
    "            bn,\n",
    "            init,\n",
    "            conv=nn.Conv1d,\n",
    "            batch_norm=BatchNorm1d,\n",
    "            bias=bias,\n",
    "            preact=preact,\n",
    "            name=name,\n",
    "            instance_norm=instance_norm,\n",
    "            instance_norm_func=nn.InstanceNorm1d\n",
    "        )\n",
    "\n",
    "\n",
    "class Conv2d(_ConvBase):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_size: int,\n",
    "            out_size: int,\n",
    "            *,\n",
    "            kernel_size: Tuple[int, int] = (1, 1),\n",
    "            stride: Tuple[int, int] = (1, 1),\n",
    "            padding: Tuple[int, int] = (0, 0),\n",
    "            activation=nn.ReLU(inplace=True),\n",
    "            bn: bool = False,\n",
    "            init=nn.init.kaiming_normal_,\n",
    "            bias: bool = True,\n",
    "            preact: bool = False,\n",
    "            name: str = \"\",\n",
    "            instance_norm=False\n",
    "    ):\n",
    "        super().__init__(\n",
    "            in_size,\n",
    "            out_size,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            activation,\n",
    "            bn,\n",
    "            init,\n",
    "            conv=nn.Conv2d,\n",
    "            batch_norm=BatchNorm2d,\n",
    "            bias=bias,\n",
    "            preact=preact,\n",
    "            name=name,\n",
    "            instance_norm=instance_norm,\n",
    "            instance_norm_func=nn.InstanceNorm2d\n",
    "        )\n",
    "\n",
    "\n",
    "class FC(nn.Sequential):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_size: int,\n",
    "            out_size: int,\n",
    "            *,\n",
    "            activation=nn.ReLU(inplace=True),\n",
    "            bn: bool = False,\n",
    "            init=None,\n",
    "            preact: bool = False,\n",
    "            name: str = \"\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        fc = nn.Linear(in_size, out_size, bias=not bn)\n",
    "        if init is not None:\n",
    "            init(fc.weight)\n",
    "        if not bn:\n",
    "            nn.init.constant(fc.bias, 0)\n",
    "\n",
    "        if preact:\n",
    "            if bn:\n",
    "                self.add_module(name + 'bn', BatchNorm1d(in_size))\n",
    "\n",
    "            if activation is not None:\n",
    "                self.add_module(name + 'activation', activation)\n",
    "\n",
    "        self.add_module(name + 'fc', fc)\n",
    "\n",
    "        if not preact:\n",
    "            if bn:\n",
    "                self.add_module(name + 'bn', BatchNorm1d(out_size))\n",
    "\n",
    "            if activation is not None:\n",
    "                self.add_module(name + 'activation', activation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
